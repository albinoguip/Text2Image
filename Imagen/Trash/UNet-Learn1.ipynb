{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "763e28d4-1f09-426d-b5a5-bc9d2c10dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import BasicModels as BM\n",
    "from ComplexModels import TextConditioning, TimeConditioning\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ba5a55-9cac-4961-a918-05112d6fe776",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cond = TimeConditioning(dim=20, cond_dim=20, time_embedding_dim=10, num_time_tokens = 68)\n",
    "\n",
    "text_cond = TextConditioning(dim=20, cond_dim=20, text_embed_dim=10, dim_head=64, heads=8,\n",
    "                             num_latents=64, max_text_len=15, Ttype=torch.float, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e36f4b1-7b33-4fe7-810f-462f0cc4ca25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 68, 20]) torch.Size([4, 80])\n",
      "torch.Size([4, 68, 20]) torch.Size([4, 80])\n"
     ]
    }
   ],
   "source": [
    "text_embeds = torch.randn(4, 15, 10)\n",
    "text_masks  = torch.ones(4, 15).bool()\n",
    "time        = torch.rand(4)\n",
    "\n",
    "time_tokens, t = time_cond(time)\n",
    "print(time_tokens.shape, t.shape)\n",
    "\n",
    "text_tokens, text_hiddens = text_cond(text_embeds, text_masks)\n",
    "print(text_tokens.shape, text_hiddens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a6ab64b3-a7e1-42b2-b4a8-75c91ef4515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import T5Tokenizer, T5EncoderModel, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f98a583f-313e-4df9-9b16-a30dcf0b5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T5_CONFIGS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "163617ae-a8f1-42d8-8d62-8e8570ec227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_encoded_dim(name='google/t5-v1_1-base'):\n",
    "    if name not in T5_CONFIGS:\n",
    "        print('1')\n",
    "        # avoids loading the model if we only want to get the dim\n",
    "        config = T5Config.from_pretrained(name)\n",
    "        T5_CONFIGS[name] = dict(config=config)\n",
    "    elif \"config\" in T5_CONFIGS[name]:\n",
    "        print('2')\n",
    "        config = T5_CONFIGS[name][\"config\"]\n",
    "    elif \"model\" in T5_CONFIGS[name]:\n",
    "        print('3')\n",
    "        config = T5_CONFIGS[name][\"model\"].config\n",
    "    else:\n",
    "        print('4')\n",
    "        assert False\n",
    "    return config.d_model\n",
    "\n",
    "get_encoded_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "96b616c1-be79-44e9-a26b-a06e1431a731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T5Config.from_pretrained('google/t5-v1_1-base').d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "989153b5-0b76-44d4-83d7-6a62cdf6afbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'google/t5-v1_1-base': {'config': T5Config {\n",
       "    \"_name_or_path\": \"/home/patrick/hugging_face/t5/t5-v1_1-base\",\n",
       "    \"architectures\": [\n",
       "      \"T5ForConditionalGeneration\"\n",
       "    ],\n",
       "    \"d_ff\": 2048,\n",
       "    \"d_kv\": 64,\n",
       "    \"d_model\": 768,\n",
       "    \"decoder_start_token_id\": 0,\n",
       "    \"dropout_rate\": 0.1,\n",
       "    \"eos_token_id\": 1,\n",
       "    \"feed_forward_proj\": \"gated-gelu\",\n",
       "    \"initializer_factor\": 1.0,\n",
       "    \"is_encoder_decoder\": true,\n",
       "    \"layer_norm_epsilon\": 1e-06,\n",
       "    \"model_type\": \"t5\",\n",
       "    \"num_decoder_layers\": 12,\n",
       "    \"num_heads\": 12,\n",
       "    \"num_layers\": 12,\n",
       "    \"output_past\": true,\n",
       "    \"pad_token_id\": 0,\n",
       "    \"relative_attention_num_buckets\": 32,\n",
       "    \"tie_word_embeddings\": false,\n",
       "    \"transformers_version\": \"4.3.0\",\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 32128\n",
       "  }}}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T5_CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b9dea75-9a93-441f-97f1-6325f72a382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x\n",
    "    \n",
    "class Parallel(nn.Module):\n",
    "    def __init__(self, *fns):\n",
    "        super().__init__()\n",
    "        self.fns = nn.ModuleList(fns)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [fn(x) for fn in self.fns]\n",
    "        return sum(outputs)\n",
    "    \n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f1b0e0bb-069b-4064-96ec-d5cd60669fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        image_embed_dim = 1024,\n",
    "        text_embed_dim = 40, # get_encoded_dim(DEFAULT_T5_NAME),\n",
    "        num_resnet_blocks = 1,\n",
    "        cond_dim = None,\n",
    "        num_image_tokens = 4,\n",
    "        num_time_tokens = 2,\n",
    "        learned_sinu_pos_emb = True,\n",
    "        learned_sinu_pos_emb_dim = 16,\n",
    "        out_dim = None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        cond_images_channels = 0,\n",
    "        channels = 3,\n",
    "        channels_out = None,\n",
    "        attn_dim_head = 64,\n",
    "        attn_heads = 8,\n",
    "        ff_mult = 2.,\n",
    "        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
    "        layer_attns = True,\n",
    "        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
    "        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
    "        layer_cross_attns = True,\n",
    "        use_linear_attn = False,\n",
    "        use_linear_cross_attn = False,\n",
    "        cond_on_text = True,\n",
    "        max_text_len = 256,\n",
    "        init_dim = None,\n",
    "        init_conv_kernel_size = 7,\n",
    "        resnet_groups = 8,\n",
    "        init_cross_embed_kernel_sizes = (3, 7, 15),\n",
    "        cross_embed_downsample = False,\n",
    "        cross_embed_downsample_kernel_sizes = (2, 4),\n",
    "        attn_pool_text = True,\n",
    "        attn_pool_num_latents = 32,\n",
    "        dropout = 0.,\n",
    "        memory_efficient = False,\n",
    "        init_conv_to_final_conv_residual = False,\n",
    "        use_global_context_attn = True,\n",
    "        scale_skip_connection = True,\n",
    "        final_resnet_block = True,\n",
    "        final_conv_kernel_size = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_cond = TimeConditioning(dim                = dim,\n",
    "                                          cond_dim           = cond_dim,\n",
    "                                          time_embedding_dim = learned_sinu_pos_emb_dim,\n",
    "                                          num_time_tokens    = num_time_tokens)\n",
    "\n",
    "        self.text_cond = TextConditioning(dim            = dim,\n",
    "                                          cond_dim       = cond_dim,\n",
    "                                          text_embed_dim = text_embed_dim,\n",
    "                                          dim_head       = attn_dim_head,\n",
    "                                          heads          = attn_heads,\n",
    "                                          num_latents    = attn_pool_num_latents,\n",
    "                                          max_text_len   = max_text_len,\n",
    "                                          Ttype          = torch.float,\n",
    "                                          device         = 'cpu')\n",
    "        \n",
    "        self.norm_cond = nn.LayerNorm(cond_dim)\n",
    "        \n",
    "        self.init_conv = BM.CrossEmbedding(channels, dim, stride=1, kernel_sizes=init_cross_embed_kernel_sizes)\n",
    "        \n",
    "        # Params for UNet\n",
    "        \n",
    "        dims   = [dim, *[m*dim for m in dim_mults]]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        \n",
    "        num_resnet_blocks = (num_resnet_blocks,)*len(in_out)\n",
    "        resnet_groups     = (resnet_groups,)*len(in_out)\n",
    "        \n",
    "        assert len(num_resnet_blocks) == len(in_out), 'num_resnet_blocks and in_out must be the same size'\n",
    "        assert len(resnet_groups) == len(in_out),     'resnet_groups and in_out must be the same size'\n",
    "        assert len(layer_attns) == len(in_out),       'layer_attns and in_out must be the same size'\n",
    "        assert len(layer_cross_attns) == len(in_out), 'layer_cross_attns and in_out must be the same size'\n",
    "        \n",
    "        params   = [in_out, num_resnet_blocks, resnet_groups, layer_attns, layer_cross_attns]\n",
    "        r_params = [reversed(in_out), reversed(num_resnet_blocks), reversed(resnet_groups), reversed(layer_attns), reversed(layer_cross_attns)]\n",
    "        \n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups   = nn.ModuleList([])\n",
    "        \n",
    "        skip_connect_dims = []\n",
    "        \n",
    "        # UNet Encoder ==========================================================================================================================\n",
    "\n",
    "        for i, ((dim_in, dim_out), resnet_n, groups, layer_attn, layer_cross_attn) in enumerate(zip(*params)):\n",
    "            \n",
    "            is_last = i >= (len(in_out) - 1)\n",
    "            \n",
    "            layer_cond_dim = cond_dim if layer_cross_attn else None            \n",
    "            current_dim    = dim_in\n",
    "            \n",
    "            skip_connect_dims.append(current_dim)           \n",
    "             \n",
    "            # First Resnet\n",
    "            init_resnet = BM.ResnetLayer(current_dim, current_dim, cond_dim=layer_cond_dim,\n",
    "                                         time_cond_dim=dim*4, groups=groups, linear_att=False, gca=False)\n",
    "            \n",
    "            # Multiples Resnets\n",
    "            mult_resnet = nn.ModuleList([BM.ResnetLayer(current_dim, current_dim, time_cond_dim=dim*4,\n",
    "                                                        groups=groups, linear_att=False, gca=use_global_context_attn) for _ in range(resnet_n)])\n",
    "            \n",
    "            # Transformer Layer\n",
    "            if layer_attn:\n",
    "                print(type(current_dim), type(attn_heads), type(attn_dim_head), type(ff_mult), type('normal'), type(cond_dim))\n",
    "                \n",
    "                transformerLayer = BM.TransformerLayer(dim=current_dim, heads=attn_heads, dim_head=attn_dim_head,\n",
    "                                                       ff_mult=ff_mult, att_type='normal', context_dim=cond_dim)\n",
    "            else: \n",
    "                transformerLayer = Identity()\n",
    "                \n",
    "            # Downsample\n",
    "            if not is_last: \n",
    "                downsample = nn.Conv2d(current_dim, dim_out, 4, 2, 1)\n",
    "            else:\n",
    "                downsample = Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n",
    "                \n",
    "            # Append self.downs for Encoder\n",
    "            self.downs.append(nn.ModuleList([init_resnet, mult_resnet, transformerLayer, downsample]))\n",
    "            \n",
    "        # UNet Bottleneck =======================================================================================================================\n",
    "        \n",
    "        mid_dim = dims[-1]\n",
    "        \n",
    "        self.mid_block1 = BM.ResnetLayer(mid_dim, mid_dim, cond_dim=cond_dim, time_cond_dim=dim*4, groups=resnet_groups[-1])\n",
    "        self.mid_attn   = BM.AttentionTypes(mid_dim, heads=attn_heads, dim_head=attn_dim_head)\n",
    "        self.mid_block2 = BM.ResnetLayer(mid_dim, mid_dim, cond_dim=cond_dim, time_cond_dim=dim*4, groups=resnet_groups[-1])\n",
    "        \n",
    "        # UNet Decoder ==========================================================================================================================\n",
    "            \n",
    "        for i, ((dim_in, dim_out), resnet_n, groups, layer_attn, layer_cross_attn) in enumerate(zip(*r_params)):\n",
    "            \n",
    "            is_last = i == (len(in_out) - 1)\n",
    "            \n",
    "            layer_cond_dim = cond_dim if layer_cross_attn else None           \n",
    "            \n",
    "            \n",
    "            skip_connect_dim = skip_connect_dims.pop()\n",
    "            # First Resnet\n",
    "            # ResnetBlock(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = False, time_cond_dim = time_cond_dim, groups = groups)\n",
    "            init_resnet = BM.ResnetLayer(dim_out + skip_connect_dim, dim_out, cond_dim=layer_cond_dim,\n",
    "                                         time_cond_dim=dim*4, groups=groups, linear_att=False, gca=False)\n",
    "            \n",
    "            # Multiples Resnets\n",
    "            # nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim,\n",
    "            #                            groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)])\n",
    "            mult_resnet = nn.ModuleList([BM.ResnetLayer(dim_out + skip_connect_dim, dim_out, time_cond_dim=dim*4,\n",
    "                                                        groups=groups, linear_att=False, gca=use_global_context_attn) for _ in range(resnet_n)])\n",
    "            \n",
    "            # Transformer Layer\n",
    "            # transformer_block_klass(dim = dim_out, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult, context_dim = cond_dim)\n",
    "            # transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n",
    "            if layer_attn:\n",
    "                print(type(current_dim), type(attn_heads), type(attn_dim_head), type(ff_mult), type('normal'), type(cond_dim))\n",
    "                \n",
    "                transformerLayer = BM.TransformerLayer(dim=dim_out, heads=attn_heads, dim_head=attn_dim_head,\n",
    "                                                       ff_mult=ff_mult, att_type='normal', context_dim=cond_dim)\n",
    "            else: \n",
    "                transformerLayer = Identity()\n",
    "                \n",
    "            # Upsample\n",
    "            # Upsample(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n",
    "            if not is_last: \n",
    "                upsample = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                                         nn.Conv2d(dim_out, dim_in, 3, padding=1))\n",
    "            else:\n",
    "                upsample = Identity()               \n",
    "\n",
    "            # Append self.ups for Decoder\n",
    "            self.ups.append(nn.ModuleList([init_resnet, mult_resnet, transformerLayer, upsample]))\n",
    "            \n",
    "        # Final Layers ==========================================================================================================================\n",
    "\n",
    "        self.final_resnet = BM.ResnetLayer(dim, dim, time_cond_dim=dim*4, groups=resnet_groups[0], linear_att=False, gca=True)\n",
    "        self.final_conv = nn.Conv2d(dim, channels, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x, time, text_embeds, text_mask):\n",
    "        \n",
    "        # Time Conditioning        \n",
    "        time_tokens, t = self.time_cond(time)\n",
    "        print(time_tokens.shape, t.shape)\n",
    "\n",
    "        # Text Conditioning   \n",
    "        text_tokens, text_hiddens = self.text_cond(text_embeds, text_mask)\n",
    "        print(text_tokens.shape, text_hiddens.shape)\n",
    "        \n",
    "        \n",
    "        # Concatenating Time and Text\n",
    "\n",
    "        c = time_tokens if text_tokens is None else torch.cat((time_tokens, text_tokens), dim = -2)\n",
    "        c = self.norm_cond(c)\n",
    "        \n",
    "        t = t + text_hiddens\n",
    "        \n",
    "        # Processing Image\n",
    "        \n",
    "        x = self.init_conv(x)\n",
    "        \n",
    "        # Encoder\n",
    "        \n",
    "        hiddens = []\n",
    "        i = 0\n",
    "        for init_resnet, mult_resnet, transformerLayer, downsample in self.downs:\n",
    "            print(i)\n",
    "            x = init_resnet(x, t, c)\n",
    "\n",
    "            for resnet in mult_resnet:\n",
    "                x = resnet(x, t)\n",
    "                hiddens.append(x)\n",
    "\n",
    "            print(x.shape, c.shape)\n",
    "                \n",
    "            x = transformerLayer(x, c)\n",
    "            hiddens.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "            i += 1\n",
    "            \n",
    "        print(x.shape)\n",
    "            \n",
    "        x = self.mid_block1(x, t, c)\n",
    "        \n",
    "        w = x.shape[-1]\n",
    "        \n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')        \n",
    "        x = self.mid_attn(x) + x\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w',w=w)\n",
    "\n",
    "        x = self.mid_block2(x, t, c)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        # add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n",
    "\n",
    "        for init_resnet, mult_resnet, transformerLayer, upsample in self.ups:\n",
    "            pp = hiddens.pop()\n",
    "            print('pp', pp.shape)\n",
    "            print('x', x.shape)\n",
    "            x = torch.cat((x, pp * (2 ** -0.5)), dim = 1) # add_skip_connection(x)\n",
    "            \n",
    "            x = init_resnet(x, t, c)\n",
    "\n",
    "            for resnet in mult_resnet:\n",
    "                x = torch.cat((x, hiddens.pop() * (2 ** -0.5)), dim = 1) # add_skip_connection(x)\n",
    "                x = resnet(x, t)\n",
    "\n",
    "            x = transformerLayer(x, c)\n",
    "            x = upsample(x)\n",
    "            \n",
    "        x = self.final_resnet(x)\n",
    "        print('x1', x.shape)\n",
    "        x = self.final_conv(x)\n",
    "        print('x2', x.shape)\n",
    "        \n",
    "        return x, c, t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cf316666-7aad-48ac-b8e7-6dd2de52adf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c318e1ea-a557-4da7-af71-28da35874170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'> <class 'int'> <class 'int'> <class 'float'> <class 'str'> <class 'int'>\n",
      "<class 'int'> <class 'int'> <class 'int'> <class 'float'> <class 'str'> <class 'int'>\n",
      "<class 'int'> <class 'int'> <class 'int'> <class 'float'> <class 'str'> <class 'int'>\n",
      "<class 'int'> <class 'int'> <class 'int'> <class 'float'> <class 'str'> <class 'int'>\n",
      "<class 'int'> <class 'int'> <class 'int'> <class 'float'> <class 'str'> <class 'int'>\n",
      "<class 'int'> <class 'int'> <class 'int'> <class 'float'> <class 'str'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "u = Unet(\n",
    "    dim = 8,\n",
    "    cond_dim = 40,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "# print(type(current_dim), type(attn_heads), type(attn_dim_head), type(ff_mult), type('normal'), ctype(cond_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "937c3e25-af53-499d-bd3f-202124d7513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x           = torch.randn(4, 3, 32, 32)\n",
    "text_embeds = torch.randn(4, 32, 40)\n",
    "text_masks  = torch.ones(4, 32).bool()\n",
    "time        = torch.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0e70602f-eadb-479e-9d67-76b7619352ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 40]) torch.Size([4, 32])\n",
      "torch.Size([4, 36, 40]) torch.Size([4, 32])\n",
      "0\n",
      "torch.Size([4, 8, 32, 32]) torch.Size([4, 38, 40])\n",
      "1\n",
      "torch.Size([4, 8, 16, 16]) torch.Size([4, 38, 40])\n",
      "2\n",
      "torch.Size([4, 16, 8, 8]) torch.Size([4, 38, 40])\n",
      "3\n",
      "torch.Size([4, 32, 4, 4]) torch.Size([4, 38, 40])\n",
      "torch.Size([4, 64, 4, 4])\n",
      "torch.Size([4, 64, 4, 4])\n",
      "pp torch.Size([4, 32, 4, 4])\n",
      "x torch.Size([4, 64, 4, 4])\n",
      "pp torch.Size([4, 16, 8, 8])\n",
      "x torch.Size([4, 32, 8, 8])\n",
      "pp torch.Size([4, 8, 16, 16])\n",
      "x torch.Size([4, 16, 16, 16])\n",
      "pp torch.Size([4, 8, 32, 32])\n",
      "x torch.Size([4, 8, 32, 32])\n",
      "x1 torch.Size([4, 8, 32, 32])\n",
      "x2 torch.Size([4, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "x, c, t = u(x, time, text_embeds, text_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3e7a7370-7b5f-4141-b1f1-8e90f00088ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "torch.Size([4, 38, 40])\n",
      "torch.Size([4, 32])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(c.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b012f-962f-4c70-b6da-552e27c1899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_mults=(1, 2, 4, 8)\n",
    "init_dim=32\n",
    "dim=32\n",
    "\n",
    "dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "in_out = list(zip(dims[:-1], dims[1:]))\n",
    "in_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d812afd-0499-4f9c-9d40-fd910e2670c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [dim, *[m*dim for m in dim_mults]]\n",
    "in_out = list(zip(dims[:-1], dims[1:]))\n",
    "in_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aefcbace-ddce-40b9-ba81-9adf3359c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 10 1 -1\n",
      "1 2 20 4 -2\n",
      "2 30 300 5 -3\n"
     ]
    }
   ],
   "source": [
    "in_out = [(1, 10), (2, 20), (30, 300)]\n",
    "a = [1, 4, 5, 6]\n",
    "b = [-1, -2, -3]\n",
    "\n",
    "layer_params = [in_out, a, b]\n",
    "\n",
    "for i, ((o, oo), a1, b2) in enumerate(zip(*layer_params)):\n",
    "    print(i, o, oo, a1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cbc221f-2b3d-477f-b3c1-d3d971c929c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<list_reverseiterator at 0x25502238280>,\n",
       " <list_reverseiterator at 0x25502238340>,\n",
       " <list_reverseiterator at 0x255022383d0>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_layer_params = list(map(reversed, layer_params))\n",
    "reversed_layer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c83b3545-33c9-4093-a5b2-b66d5686a057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30 300 6 -3\n",
      "1 2 20 5 -2\n",
      "2 1 10 4 -1\n"
     ]
    }
   ],
   "source": [
    "for i, ((o, oo), a1, b2) in enumerate(zip(*reversed_layer_params)):\n",
    "    print(i, o, oo, a1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4fbab33-b4e8-4af8-907c-fcd6128dc0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30 300 6 -3\n",
      "1 2 20 5 -2\n",
      "2 1 10 4 -1\n"
     ]
    }
   ],
   "source": [
    "in_out = [(1, 10), (2, 20), (30, 300)]\n",
    "a = [1, 4, 5, 6]\n",
    "b = [-1, -2, -3]\n",
    "\n",
    "layer_params = [reversed(in_out), reversed(a), reversed(b)]\n",
    "\n",
    "for i, ((o, oo), a1, b2) in enumerate(zip(*layer_params)):\n",
    "    print(i, o, oo, a1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bfe31-e3b5-4460-852c-8d0bdc8488de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
