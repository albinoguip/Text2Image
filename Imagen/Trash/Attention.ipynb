{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960e6e0-b425-4601-ab8c-ed28d1224f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        causal = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.causal = causal\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias = False),\n",
    "            LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask = None, attn_bias = None):\n",
    "        b, n, device = *x.shape[:2], x.device\n",
    "\n",
    "        x = self.norm(x)\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n",
    "\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n",
    "        q = q * self.scale\n",
    "\n",
    "        # add null key / value for classifier free guidance in prior net\n",
    "\n",
    "        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n",
    "        k = torch.cat((nk, k), dim = -2)\n",
    "        v = torch.cat((nv, v), dim = -2)\n",
    "\n",
    "        # calculate query / key similarities\n",
    "\n",
    "        sim = einsum('b h i d, b j d -> b h i j', q, k)\n",
    "\n",
    "        # relative positional encoding (T5 style)\n",
    "\n",
    "        if exists(attn_bias):\n",
    "            sim = sim + attn_bias\n",
    "\n",
    "        # masking\n",
    "\n",
    "        max_neg_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = F.pad(mask, (1, 0), value = True)\n",
    "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
    "            sim = sim.masked_fill(~mask, max_neg_value)\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
    "\n",
    "        # aggregate values\n",
    "\n",
    "        out = einsum('b h i j, b j d -> b h i d', attn, v)\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd1d1e-11ea-406f-8945-9a1ffa714482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        context_dim = None,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        norm_context = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        context_dim = default(context_dim, dim)\n",
    "\n",
    "        self.norm = LayerNorm(dim)\n",
    "        self.norm_context = LayerNorm(context_dim) if norm_context else nn.Identity()\n",
    "\n",
    "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias = False),\n",
    "            LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context, mask = None):\n",
    "        b, n, device = *x.shape[:2], x.device\n",
    "\n",
    "        x = self.norm(x)\n",
    "        context = self.norm_context(context)\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n",
    "\n",
    "        # add null key / value for classifier free guidance in prior net\n",
    "\n",
    "        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n",
    "\n",
    "        k = torch.cat((nk, k), dim = -2)\n",
    "        v = torch.cat((nv, v), dim = -2)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "        max_neg_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = F.pad(mask, (1, 0), value = True)\n",
    "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
    "            sim = sim.masked_fill(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa603f5a-75c7-4406-b733-23275e511c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCrossAttention(CrossAttention):\n",
    "    def forward(self, x, context, mask = None):\n",
    "        b, n, device = *x.shape[:2], x.device\n",
    "\n",
    "        x = self.norm(x)\n",
    "        context = self.norm_context(context)\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n",
    "\n",
    "        # add null key / value for classifier free guidance in prior net\n",
    "\n",
    "        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n",
    "\n",
    "        k = torch.cat((nk, k), dim = -2)\n",
    "        v = torch.cat((nv, v), dim = -2)\n",
    "\n",
    "        # masking\n",
    "\n",
    "        max_neg_value = -torch.finfo(x.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = F.pad(mask, (1, 0), value = True)\n",
    "            mask = rearrange(mask, 'b n -> b n 1')\n",
    "            k = k.masked_fill(~mask, max_neg_value)\n",
    "            v = v.masked_fill(~mask, 0.)\n",
    "\n",
    "        # linear attention\n",
    "\n",
    "        q = q.softmax(dim = -1)\n",
    "        k = k.softmax(dim = -2)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        context = einsum('b n d, b n e -> b d e', k, v)\n",
    "        out = einsum('b n d, b d e -> b n e', q, context)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19786ef-f5a2-4a23-a16e-6a2cae971376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_head = 32,\n",
    "        heads = 8,\n",
    "        dropout = 0.05\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "        self.norm = ChanLayerNorm(dim)\n",
    "\n",
    "        self.nonlin = nn.SiLU()\n",
    "\n",
    "        self.to_q = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
    "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
    "        )\n",
    "\n",
    "        self.to_k = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
    "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
    "        )\n",
    "\n",
    "        self.to_v = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
    "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
    "        )\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(inner_dim, dim, 1, bias = False),\n",
    "            ChanLayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        h, x, y = self.heads, *fmap.shape[-2:]\n",
    "\n",
    "        fmap = self.norm(fmap)\n",
    "        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n",
    "        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n",
    "\n",
    "        q = q.softmax(dim = -1)\n",
    "        k = k.softmax(dim = -2)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        context = einsum('b n d, b n e -> b d e', k, v)\n",
    "        out = einsum('b n d, b d e -> b n e', q, context)\n",
    "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n",
    "\n",
    "        out = self.nonlin(out)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8eecac-9973-4a09-a420-ab3ec3e8e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        heads = 8,\n",
    "        dim_head = 32,\n",
    "        ff_mult = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn = EinopsToAndFrom('b c h w', 'b (h w) c', Attention(dim = dim, heads = heads, dim_head = dim_head))\n",
    "        self.ff = ChanFeedForward(dim = dim, mult = ff_mult)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(x) + x\n",
    "        x = self.ff(x) + x\n",
    "        return x\n",
    "\n",
    "class LinearAttentionTransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        heads = 8,\n",
    "        dim_head = 32,\n",
    "        ff_mult = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn = LinearAttention(dim = dim, heads = heads, dim_head = dim_head)\n",
    "        self.ff = ChanFeedForward(dim = dim, mult = ff_mult)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(x) + x\n",
    "        x = self.ff(x) + x\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
