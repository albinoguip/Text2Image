{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be11003e-3b3d-485d-9fb1-ef207cd28283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load UNet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "#######################################################################\n",
    "#################### CLASSES FOR TIME CONDITIONING ####################\n",
    "#######################################################################\n",
    "\n",
    "class SinusoidalPositionEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        \n",
    "        super(SinusoidalPositionEmbedding, self).__init__()\n",
    "        assert (dim % 2) == 0\n",
    "        self.weights = nn.Parameter(torch.randn(dim // 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = rearrange(x, 'b -> b 1')    \n",
    "        f = x * rearrange(self.weights, 'd -> 1 d') \n",
    "        w = f * 2 * torch.pi\n",
    "        print(torch.cat((x, torch.sin(w), torch.cos(w)), dim = -1).shape)\n",
    "        return torch.cat((x, torch.sin(w), torch.cos(w)), dim = -1)\n",
    "\n",
    "class TimeConditioning(nn.Module):\n",
    "    \n",
    "    def __init__(self, unet_dim, time_embedding_dim=16, num_time_tokens = 2):\n",
    "        super(TimeConditioning, self).__init__()\n",
    "        \n",
    "        self.to_time_hiddens = nn.Sequential(SinusoidalPositionEmbedding(time_embedding_dim),\n",
    "                                             nn.Linear(time_embedding_dim+1, unet_dim*4),\n",
    "                                             Swish())\n",
    "\n",
    "        self.to_time_cond = nn.Linear(unet_dim*4, unet_dim*4)\n",
    "\n",
    "        self.to_time_tokens = nn.Sequential(nn.Linear(unet_dim*4, unet_dim * num_time_tokens),\n",
    "                                            Rearrange('b (r d) -> b r d', r = num_time_tokens))\n",
    "        \n",
    "    def forward(self, time):\n",
    "        \n",
    "        time_hiddens = self.to_time_hiddens(time)\n",
    "        print(time_hiddens.shape)\n",
    "        time_tokens = self.to_time_tokens(time_hiddens)\n",
    "        t           = self.to_time_cond(time_hiddens)\n",
    "        \n",
    "        return t, time_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b07d7598-a925-44e5-a60c-ce6db7a7df4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 11])\n",
      "torch.Size([10, 32])\n"
     ]
    }
   ],
   "source": [
    "tc = TimeConditioning(8, 10, 5)\n",
    "\n",
    "x = torch.rand((10))\n",
    "t, time_tokens = tc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd4721a8-35ee-44c1-a0e0-a0bd3b407b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65f9ea9c-0a7e-4443-bbc9-75e9436127bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 8])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988d2da-0b6f-4b05-b145-7e2ea7907456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_head = 64,\n",
    "        heads = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_latents = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias = False),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, latents, mask = None):\n",
    "        x = self.norm(x)\n",
    "        latents = self.norm_latents(latents)\n",
    "\n",
    "        b, h = x.shape[0], self.heads\n",
    "\n",
    "        q = self.to_q(latents)\n",
    "\n",
    "        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n",
    "        kv_input = torch.cat((x, latents), dim = -2)\n",
    "        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        # attention\n",
    "\n",
    "        sim = einsum('... i d, ... j d  -> ... i j', q, k)\n",
    "\n",
    "        if exists(mask):\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n",
    "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
    "            sim = sim.masked_fill(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
    "\n",
    "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a9705783-ca65-4b76-af95-41fdcfdc0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverAttention(nn.Module):\n",
    "    def __init__(self, cond_dim, dim_head = 64, heads = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.norm_x = nn.LayerNorm(cond_dim)\n",
    "        self.norm_l = nn.LayerNorm(cond_dim)\n",
    "\n",
    "        self.Q  = nn.Linear(cond_dim, dim_head * heads, bias = False)\n",
    "        self.KV = nn.Linear(cond_dim, dim_head * heads * 2, bias = False)\n",
    "\n",
    "        self.output_layer = nn.Sequential(nn.Linear(dim_head * heads, cond_dim, bias = False),\n",
    "                                          nn.LayerNorm(cond_dim))\n",
    "        \n",
    "    def attention(self, q, k, v, mask=None):\n",
    "        \n",
    "        score = torch.matmul(q, k.transpose(-1, -2))\n",
    "        \n",
    "        if mask is not None:            \n",
    "            max_neg = -torch.finfo(score.dtype).max            \n",
    "            mask    = mask[:, None, None, :]\n",
    "            score   = score.masked_fill(~mask, max_neg)\n",
    "            \n",
    "        probs = score.softmax(dim = -1, dtype = torch.float32)\n",
    "        \n",
    "        return torch.matmul(probs, v)\n",
    "\n",
    "    def forward(self, x, latents, mask = None):\n",
    "        \n",
    "        batch = x.shape[0]\n",
    "        \n",
    "        x = self.norm_x(x)\n",
    "        l = self.norm_l(latents)\n",
    "\n",
    "        q    = self.Q(l)\n",
    "        k, v = self.KV(torch.cat((x, l), dim = -2)).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n",
    "\n",
    "        q = q * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask, (0, l.shape[-2]), value = True)\n",
    "            att = self.attention(q, k, v, mask)\n",
    "        else:\n",
    "            att = self.attention(q, k, v)        \n",
    "\n",
    "        out = rearrange(att, 'b h n d -> b n (h d)', h = self.heads)\n",
    "        return self.output_layer(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22ba363e-681a-45ea-9f03-0a070dd2fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.rand((10, 4, 3))\n",
    "Q = torch.rand((10, 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4e27962-9ff3-4c65-8420-bec6367b1698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 5])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = torch.matmul(K, Q)\n",
    "pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7fce274-1e3e-4da3-9c7a-0c7644af1247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 5])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = einsum('... i j, ... j d -> ... i d', K, Q)\n",
    "pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb7dadcc-6e77-4632-945e-0fce2f259bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 10])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PA = PerceiverAttention(cond_dim=10, dim_head = 5, heads = 4)\n",
    "\n",
    "x = torch.rand((4, 10, 10))\n",
    "m = torch.ones((4, 10), dtype=torch.bool)\n",
    "y = PA(x, x, m)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "530ddcf0-637d-4e02-a62f-2dce38890527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2584, 0.9313]]],\n",
       "\n",
       "\n",
       "        [[[0.4733, 0.5533]]],\n",
       "\n",
       "\n",
       "        [[[0.7753, 0.6512]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.rand((3, 2))\n",
    "m[:, None, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "721dd66b-b280-447c-ac68-88111d8951ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2584, 0.9313]]],\n",
       "\n",
       "\n",
       "        [[[0.4733, 0.5533]]],\n",
       "\n",
       "\n",
       "        [[[0.7753, 0.6512]]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrange(m, 'b j -> b 1 1 j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7017a00f-3d3d-4d23-95d5-762f800e5b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "135498b8-557d-4829-a784-aa750bebeed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.5745, 1.1404, 0.4792],\n",
       "         [0.9358, 0.4613, 0.3689],\n",
       "         [1.3690, 1.1262, 0.6740],\n",
       "         [1.0577, 0.7342, 0.4657],\n",
       "         [1.1156, 0.8266, 0.6988]],\n",
       "\n",
       "        [[0.3407, 0.9385, 0.0908],\n",
       "         [0.7362, 0.9733, 0.1799],\n",
       "         [1.1299, 1.5349, 0.5749],\n",
       "         [0.8635, 1.2521, 0.1771],\n",
       "         [1.3654, 1.4977, 0.4404]],\n",
       "\n",
       "        [[0.8268, 0.9538, 0.8726],\n",
       "         [1.6497, 1.2049, 1.6238],\n",
       "         [1.3238, 1.4486, 1.4216],\n",
       "         [1.1053, 0.4499, 1.0812],\n",
       "         [1.3090, 1.1986, 1.3285]],\n",
       "\n",
       "        [[1.7625, 0.7689, 1.7370],\n",
       "         [1.1853, 0.9257, 0.9524],\n",
       "         [2.1698, 1.2840, 1.8145],\n",
       "         [1.6721, 1.0293, 1.5588],\n",
       "         [0.5936, 0.2678, 0.6266]],\n",
       "\n",
       "        [[1.3417, 1.2941, 1.1318],\n",
       "         [0.6657, 0.9366, 0.2966],\n",
       "         [0.6792, 0.8422, 0.6958],\n",
       "         [1.2936, 1.3798, 0.7328],\n",
       "         [1.3701, 1.3936, 1.4157]],\n",
       "\n",
       "        [[1.1298, 1.9560, 1.9029],\n",
       "         [1.6332, 1.8136, 1.9687],\n",
       "         [1.4505, 2.1276, 1.8893],\n",
       "         [1.5339, 1.4665, 1.6528],\n",
       "         [1.3921, 2.0863, 2.2724]],\n",
       "\n",
       "        [[0.5401, 0.7386, 0.6151],\n",
       "         [1.0068, 1.4847, 0.9891],\n",
       "         [0.9271, 1.3186, 0.7955],\n",
       "         [0.8323, 1.2219, 0.7659],\n",
       "         [1.0541, 1.3892, 1.1171]],\n",
       "\n",
       "        [[0.6157, 0.8744, 0.2071],\n",
       "         [1.0993, 0.9670, 0.6861],\n",
       "         [1.2045, 1.3269, 0.7448],\n",
       "         [1.0115, 1.2375, 0.6895],\n",
       "         [0.5675, 1.1334, 0.3034]],\n",
       "\n",
       "        [[1.1705, 0.5676, 0.2127],\n",
       "         [1.9210, 1.3104, 0.5271],\n",
       "         [1.4476, 0.6817, 0.5337],\n",
       "         [1.8370, 0.9317, 0.6285],\n",
       "         [1.8214, 1.0752, 0.4305]],\n",
       "\n",
       "        [[0.7305, 1.2067, 1.0875],\n",
       "         [0.7368, 0.9225, 0.8361],\n",
       "         [0.6483, 0.8871, 0.9378],\n",
       "         [0.6007, 0.9251, 1.0603],\n",
       "         [1.2582, 1.5279, 1.3263]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78c5175a-43bc-48fa-821d-14873622bab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.5745, 1.1404, 0.4792],\n",
       "         [0.9358, 0.4613, 0.3689],\n",
       "         [1.3690, 1.1262, 0.6740],\n",
       "         [1.0577, 0.7342, 0.4657],\n",
       "         [1.1156, 0.8266, 0.6988]],\n",
       "\n",
       "        [[0.3407, 0.9385, 0.0908],\n",
       "         [0.7362, 0.9733, 0.1799],\n",
       "         [1.1299, 1.5349, 0.5749],\n",
       "         [0.8635, 1.2521, 0.1771],\n",
       "         [1.3654, 1.4977, 0.4404]],\n",
       "\n",
       "        [[0.8268, 0.9538, 0.8726],\n",
       "         [1.6497, 1.2049, 1.6238],\n",
       "         [1.3238, 1.4486, 1.4216],\n",
       "         [1.1053, 0.4499, 1.0812],\n",
       "         [1.3090, 1.1986, 1.3285]],\n",
       "\n",
       "        [[1.7625, 0.7689, 1.7370],\n",
       "         [1.1853, 0.9257, 0.9524],\n",
       "         [2.1698, 1.2840, 1.8145],\n",
       "         [1.6721, 1.0293, 1.5588],\n",
       "         [0.5936, 0.2678, 0.6266]],\n",
       "\n",
       "        [[1.3417, 1.2941, 1.1318],\n",
       "         [0.6657, 0.9366, 0.2966],\n",
       "         [0.6792, 0.8422, 0.6958],\n",
       "         [1.2936, 1.3798, 0.7328],\n",
       "         [1.3701, 1.3936, 1.4157]],\n",
       "\n",
       "        [[1.1298, 1.9560, 1.9029],\n",
       "         [1.6332, 1.8136, 1.9687],\n",
       "         [1.4505, 2.1276, 1.8893],\n",
       "         [1.5339, 1.4665, 1.6528],\n",
       "         [1.3921, 2.0863, 2.2724]],\n",
       "\n",
       "        [[0.5401, 0.7386, 0.6151],\n",
       "         [1.0068, 1.4847, 0.9891],\n",
       "         [0.9271, 1.3186, 0.7955],\n",
       "         [0.8323, 1.2219, 0.7659],\n",
       "         [1.0541, 1.3892, 1.1171]],\n",
       "\n",
       "        [[0.6157, 0.8744, 0.2071],\n",
       "         [1.0993, 0.9670, 0.6861],\n",
       "         [1.2045, 1.3269, 0.7448],\n",
       "         [1.0115, 1.2375, 0.6895],\n",
       "         [0.5675, 1.1334, 0.3034]],\n",
       "\n",
       "        [[1.1705, 0.5676, 0.2127],\n",
       "         [1.9210, 1.3104, 0.5271],\n",
       "         [1.4476, 0.6817, 0.5337],\n",
       "         [1.8370, 0.9317, 0.6285],\n",
       "         [1.8214, 1.0752, 0.4305]],\n",
       "\n",
       "        [[0.7305, 1.2067, 1.0875],\n",
       "         [0.7368, 0.9225, 0.8361],\n",
       "         [0.6483, 0.8871, 0.9378],\n",
       "         [0.6007, 0.9251, 1.0603],\n",
       "         [1.2582, 1.5279, 1.3263]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = einsum('... i d, ... j d  -> ... i j', Q, K)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab5769b2-2426-40da-9181-ce6798bfdf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2724)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "262cb272-85cc-40d3-a880-4ea09993a928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4028234663852886e+38"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_neg_value = -torch.finfo(sim.dtype).max\n",
    "max_neg_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b478eb-37aa-4a9e-88a6-8fd46a91095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        num_latents = 64,\n",
    "        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n",
    "        max_seq_len = 512,\n",
    "        ff_mult = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
    "\n",
    "        self.to_latents_from_mean_pooled_seq = None\n",
    "\n",
    "        if num_latents_mean_pooled > 0:\n",
    "            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n",
    "                LayerNorm(dim),\n",
    "                nn.Linear(dim, dim * num_latents_mean_pooled),\n",
    "                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n",
    "            )\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n",
    "                FeedForward(dim = dim, mult = ff_mult)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        n, device = x.shape[1], x.device\n",
    "        pos_emb = self.pos_emb(torch.arange(n, device = device))\n",
    "\n",
    "        x_with_pos = x + pos_emb\n",
    "\n",
    "        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n",
    "\n",
    "        if exists(self.to_latents_from_mean_pooled_seq):\n",
    "            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n",
    "            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n",
    "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            latents = attn(x_with_pos, latents, mask = mask) + latents\n",
    "            latents = ff(latents) + latents\n",
    "\n",
    "        return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721ff68-c5ec-49db-9d13-e76a451041df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0381f-c1be-4f36-87a2-f3b15efe3e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666a0da9-52a9-4d97-8225-96edf4c9ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "cond_dim = None\n",
    "dim = 64\n",
    "\n",
    "cond_dim = default(cond_dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51957c93-9558-4465-b398-389f22fec59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db56d08f-b6b0-4dcc-a248-9e3d80ce9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange, repeat\n",
    "from einops_exts import rearrange_many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e50f57de-99d7-4fc7-bf3f-426e013118f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverAttention(nn.Module):\n",
    "    def __init__(self, cond_dim, dim_head = 64, heads = 8):\n",
    "        super(PerceiverAttention, self).__init__()\n",
    "        \n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.norm_x = nn.LayerNorm(cond_dim)\n",
    "        self.norm_l = nn.LayerNorm(cond_dim)\n",
    "\n",
    "        self.Q  = nn.Linear(cond_dim, dim_head * heads, bias = False)\n",
    "        self.KV = nn.Linear(cond_dim, dim_head * heads * 2, bias = False)\n",
    "\n",
    "        self.output_layer = nn.Sequential(nn.Linear(dim_head * heads, cond_dim, bias = False),\n",
    "                                          nn.LayerNorm(cond_dim))\n",
    "        \n",
    "    def attention(self, q, k, v, mask=None):\n",
    "        \n",
    "        score = torch.matmul(q, k.transpose(-1, -2))\n",
    "        \n",
    "        if mask is not None:            \n",
    "            max_neg = -torch.finfo(score.dtype).max            \n",
    "            mask    = mask[:, None, None, :]\n",
    "            score   = score.masked_fill(~mask, max_neg)\n",
    "            \n",
    "        probs = score.softmax(dim = -1, dtype = torch.float32)\n",
    "        \n",
    "        return torch.matmul(probs, v)\n",
    "\n",
    "    def forward(self, x, latents, mask = None):\n",
    "        \n",
    "        batch = x.shape[0]\n",
    "        \n",
    "        x = self.norm_x(x)\n",
    "        l = self.norm_l(latents)\n",
    "\n",
    "        q    = self.Q(l)\n",
    "        k, v = self.KV(torch.cat((x, l), dim = -2)).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n",
    "\n",
    "        q = q * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask, (0, l.shape[-2]), value = True)\n",
    "            att = self.attention(q, k, v, mask)\n",
    "        else:\n",
    "            att = self.attention(q, k, v)        \n",
    "\n",
    "        out = rearrange(att, 'b h n d -> b n (h d)', h = self.heads)\n",
    "        return self.output_layer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7489ba0-2ff7-4009-abfb-bc411ea05331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c132ba9e-e1f6-4bbb-90c6-fd86f69e057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterPerceiver(nn.Module):\n",
    "    def __init__(self, cond_dim, depth, dim_head = 64, heads = 8, num_latents = 64,\n",
    "                 num_latents_mean_pooled = 4, max_seq_len = 512, ff_mult = 4):        \n",
    "        # num_latents_mean_pooled: number of latents derived from mean pooled representation of the sequence\n",
    "        \n",
    "        super(MasterPerceiver, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_seq_len, cond_dim)\n",
    "        self.latents   = nn.Parameter(torch.randn(num_latents, cond_dim))\n",
    "\n",
    "        self.Mean_Pooled = None\n",
    "        if num_latents_mean_pooled > 0:\n",
    "            self.Mean_Pooled = nn.Sequential(nn.LayerNorm(cond_dim),\n",
    "                                             nn.Linear(cond_dim, cond_dim * num_latents_mean_pooled),\n",
    "                                             Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled))\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([PerceiverAttention(cond_dim, dim_head, heads),\n",
    "                               nn.Sequential(nn.LayerNorm(cond_dim),\n",
    "                                             nn.Linear(cond_dim, cond_dim*ff_mult, bias = False),\n",
    "                                             Swish(),\n",
    "                                             nn.LayerNorm(cond_dim*ff_mult),\n",
    "                                             nn.Linear(cond_dim*ff_mult, cond_dim, bias = False))]))\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        \n",
    "        b, n, device = x.shape[0], x.shape[1], x.device\n",
    "        \n",
    "        pos_emb = self.embedding(torch.arange(n, device = device))\n",
    "        X = x + pos_emb\n",
    "\n",
    "        lat = repeat(self.latents, 'n d -> b n d', b = b)\n",
    "\n",
    "        if self.Mean_Pooled is not None:\n",
    "                \n",
    "            pool_mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool)\n",
    "                \n",
    "            denom     = pool_mask.sum(dim = 1, keepdim = True)\n",
    "            pool_mask = pool_mask[:, :, None]\n",
    "            masked_x  = x.masked_fill(~pool_mask, 0.)\n",
    "            \n",
    "            m_seq = masked_x.sum(dim = 1) / denom.clamp(min = 1e-5)\n",
    "            m_lat = self.Mean_Pooled(m_seq)\n",
    "            lat   = torch.cat((m_lat, lat), dim = -2)\n",
    "            \n",
    "            print(lat.shape)\n",
    "\n",
    "        for att, nn in self.layers:\n",
    "            lat = nn(att(X, lat, mask = mask) + lat) + lat\n",
    "\n",
    "        return lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d097f699-4872-403f-94d8-7148abced9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MasterPerceiver(cond_dim=20, depth=2, dim_head = 64, heads = 8, num_latents = 64,\n",
    "                 num_latents_mean_pooled = 4, max_seq_len = 15, ff_mult = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5be67828-9c66-46d0-be97-2d9b0318de51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 68, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 68, 20])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((4, 15, 20))\n",
    "m = torch.ones((4, 15), dtype=torch.bool)\n",
    "y = mp(x, m)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a52b2a-2655-44ab-815c-138c7cc5b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = PerceiverAttention(cond_dim=10, dim_head = 5, heads = 4)\n",
    "\n",
    "x = torch.rand((4, 10, 10))\n",
    "m = torch.ones((4, 10), dtype=torch.bool)\n",
    "y = PA(x, x, m)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a495ebc-771b-4d4e-ad43-70ed41280850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9309, 0.9206]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.rand((2))\n",
    "rearrange(m, 'b -> 1 b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d43394bb-a528-43d1-b51d-afd08e2eacc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9309, 0.9206]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15cb0b8-b2ee-4324-93d5-5cb338af95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean(t, *, dim, mask = None):\n",
    "    if not exists(mask):\n",
    "        return t.mean(dim = dim)\n",
    "\n",
    "    denom = mask.sum(dim = dim, keepdim = True)\n",
    "    mask = rearrange(mask, 'b n -> b n 1')\n",
    "    masked_t = t.masked_fill(~mask, 0.)\n",
    "\n",
    "    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a05273b3-0198-43d6-83fe-daa15224ed8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "b = 8\n",
    "\n",
    "print(b is not None)\n",
    "print(b is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0d3ff65-f8ca-41f7-b8e4-113ae7a6a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cu():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.me()\n",
    "        \n",
    "    def me(self):\n",
    "        print('cu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ffb0caa8-c583-4305-85b1-05c8e86b18d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu\n"
     ]
    }
   ],
   "source": [
    "c = cu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ea5cca5-53f3-4b25-9b63-3b0f1abca264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(dim, mult = 2):\n",
    "    hidden_dim = int(dim * mult)\n",
    "    return nn.Sequential(\n",
    "        LayerNorm(dim),\n",
    "        nn.Linear(dim, hidden_dim, bias = False),\n",
    "        nn.GELU(),\n",
    "        LayerNorm(hidden_dim),\n",
    "        nn.Linear(hidden_dim, dim, bias = False)\n",
    "    )\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer('beta', torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n",
    "    \n",
    "class PerceiverAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_head = 64,\n",
    "        heads = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_latents = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias = False),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, latents, mask = None):\n",
    "        x = self.norm(x)\n",
    "        latents = self.norm_latents(latents)\n",
    "\n",
    "        b, h = x.shape[0], self.heads\n",
    "\n",
    "        q = self.to_q(latents)\n",
    "\n",
    "        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n",
    "        kv_input = torch.cat((x, latents), dim = -2)\n",
    "        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        # attention\n",
    "\n",
    "        sim = einsum('... i d, ... j d  -> ... i j', q, k)\n",
    "\n",
    "        if exists(mask):\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n",
    "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
    "            sim = sim.masked_fill(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
    "\n",
    "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        num_latents = 64,\n",
    "        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n",
    "        max_seq_len = 512,\n",
    "        ff_mult = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
    "\n",
    "        self.to_latents_from_mean_pooled_seq = None\n",
    "\n",
    "        if num_latents_mean_pooled > 0:\n",
    "            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n",
    "                LayerNorm(dim),\n",
    "                nn.Linear(dim, dim * num_latents_mean_pooled),\n",
    "                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n",
    "            )\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n",
    "                FeedForward(dim = dim, mult = ff_mult)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        n, device = x.shape[1], x.device\n",
    "        pos_emb = self.pos_emb(torch.arange(n, device = device))\n",
    "\n",
    "        x_with_pos = x + pos_emb\n",
    "\n",
    "        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n",
    "        \n",
    "        print(latents.shape)\n",
    "\n",
    "        if exists(self.to_latents_from_mean_pooled_seq):\n",
    "            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n",
    "            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n",
    "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n",
    "            print(latents.shape)\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            latents = attn(x_with_pos, latents, mask = mask) + latents\n",
    "            latents = ff(latents) + latents\n",
    "            \n",
    "            print(latents.shape)\n",
    "\n",
    "        return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8af11e-6246-4cb6-90cf-45552934634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MasterPerceiver(cond_dim=20, depth=2, dim_head = 64, heads = 8, num_latents = 64,\n",
    "                 num_latents_mean_pooled = 4, max_seq_len = 15, ff_mult = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caf36f68-bcf3-409c-86f6-e3ff3200c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PerceiverResampler(dim=20, depth=2, dim_head = 64, heads = 8, num_latents = 64,\n",
    "                        num_latents_mean_pooled = 4, max_seq_len = 15, ff_mult = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5d29af2-229b-4f2e-81e3-c1dc600d9218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 20])\n",
      "torch.Size([4, 68, 20])\n",
      "torch.Size([4, 68, 20])\n",
      "torch.Size([4, 68, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 68, 20])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((4, 15, 20))\n",
    "m = torch.ones((4, 15), dtype=torch.bool)\n",
    "y = pr(x, m)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9091f292-47be-4439-a8d8-c8d5f8cc38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec9e49a5-b131-42a2-85b0-0a1c9fbd4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean(t, *, dim, mask = None):\n",
    "    if not exists(mask):\n",
    "        return t.mean(dim = dim)\n",
    "\n",
    "    denom = mask.sum(dim = dim, keepdim = True)\n",
    "    mask = rearrange(mask, 'b n -> b n 1')\n",
    "    masked_t = t.masked_fill(~mask, 0.)\n",
    "\n",
    "    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223cf39-874a-4efd-917e-f9972d61118d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9215cc8-9995-4a1b-9287-7423a11eb509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae341ac-0283-48cf-882a-7b07c2e95bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132bbc3b-a9df-4c27-95a7-59829be3e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextConditioning(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, cond_dim, text_embed_dim, dim_head, heads,\n",
    "                 num_latents, max_text_len, Ttype, device='cpu'):\n",
    "        super(TextConditioning, self).__init__()\n",
    "        \n",
    "        self.max_text_len = max_text_len\n",
    "        self.Ttype        = Ttype\n",
    "        \n",
    "        self.text_to_cond = nn.Linear(text_embed_dim, dim)\n",
    "        self.attention    = MasterPerceiver(dim=dim, depth=2, dim_head=dim_head, heads=heads, num_latents=num_latents,\n",
    "                                            num_latents_mean_pooled=4, max_seq_len=512, ff_mult=4)    \n",
    "        \n",
    "        self.null_text_embed  = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n",
    "        self.null_text_hidden = nn.Parameter(torch.randn(1, dim*4))\n",
    "        \n",
    "        self.text_to_hiddens = nn.Sequential(nn.LayerNorm(cond_dim),\n",
    "                                             nn.Linear(cond_dim, dim*4),\n",
    "                                             Swish(),\n",
    "                                              nn.Linear(dim*4, time_cond_dim))\n",
    "        \n",
    "    def forward(self, text_embeds, text_mask=None):\n",
    "        \n",
    "        # Making Masks\n",
    "        \n",
    "        text_keep_mask        = torch.zeros((batch_size,), device = device).float().uniform_(0, 1) < 0.9\n",
    "        text_keep_mask_embed  = text_keep_mask[:, None, None]\n",
    "        text_keep_mask_hidden = text_keep_mask[:, None]\n",
    "        \n",
    "        # Text Tokens\n",
    "        \n",
    "        text_tokens = self.text_to_cond(text_embeds)[:, :self.max_text_len]\n",
    "        \n",
    "        remainder = self.max_text_len - text_tokens.shape[1]\n",
    "        if remainder > 0: text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n",
    "        \n",
    "        if text_mask is not None:\n",
    "            if remainder > 0: text_mask = F.pad(text_mask, (0, remainder), value = False)\n",
    "            \n",
    "            text_mask = text_mask[:, :, None]\n",
    "            text_keep_mask_embed = text_mask & text_keep_mask_embed\n",
    "            \n",
    "        null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n",
    "\n",
    "        text_tokens = torch.where(text_keep_mask_embed, text_tokens, null_text_embed)        \n",
    "        text_tokens = self.attention(text_tokens)\n",
    "        \n",
    "        # Text Hiddens\n",
    "        \n",
    "        text_hiddens     = self.text_to_hiddens(text_tokens.mean(dim = -2))\n",
    "        null_text_hidden = self.null_text_hidden.to(self.Ttype)\n",
    "        text_hiddens     = torch.where(text_keep_mask_hidden, text_hiddens, null_text_hidden)\n",
    "        \n",
    "        \n",
    "        return text_tokens, text_hiddens\n",
    "\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
