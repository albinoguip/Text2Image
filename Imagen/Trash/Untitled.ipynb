{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774d5631-6fbb-4a13-a07c-a1f4c0f2d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple, Union, List\n",
    "\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7da8ff-35ee-42bd-bea1-009c18ce36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderT5Based():\n",
    "    \n",
    "    def __init__(self, name = 'google/t5-v1_1-small', device='cpu'):\n",
    "        \n",
    "        self.device    = device\n",
    "        self.model     = T5EncoderModel.from_pretrained(name).to(device)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(name)\n",
    "        \n",
    "    def textEncoder(self, texts):\n",
    "        \n",
    "        text_encoded = self.tokenizer.batch_encode_plus(texts, return_tensors = \"pt\", padding = 'longest',\n",
    "                                                        max_length = MAX_LENGTH, truncation = True)\n",
    "        \n",
    "        text_ids = text_encoded.input_ids.to(self.device)\n",
    "        mask     = text_encoded.attention_mask.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad(): encoded_text = self.model(text_ids, mask).last_hidden_state.detach()\n",
    "                \n",
    "        return encoded_text, mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb24789-afcf-4bf8-b688-e7c20ae2fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f5452fd-fd38-4f6c-a34f-82cd15a359e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, time_dim : int, device = 'cpu'):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "        \n",
    "        self.time_dim = time_dim\n",
    "        if time_dim >= 8 and time_dim < 16:\n",
    "            raise ValueError(f'time_dim must be: time_dim < 8 or time_dim >= 16')\n",
    "\n",
    "        self.half_dim = time_dim // 8\n",
    "        \n",
    "        ara = torch.arange(self.half_dim, device=device)\n",
    "        div = -(math.log(10_000) / (self.half_dim - 1))\n",
    "        \n",
    "        self.emb = torch.exp(ara * div)\n",
    "        \n",
    "        # Layers\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.time_dim // 4, self.time_dim)\n",
    "        self.linear2 = nn.Linear(self.time_dim, self.time_dim)\n",
    "        \n",
    "        self.swish   = Swish()\n",
    "        \n",
    "    def forward(self, t):\n",
    "        \n",
    "        out = t[:, None] * self.emb[None, :]\n",
    "        out = torch.cat((out.sin(), out.cos()), dim=1)\n",
    "        out = self.swish(self.linear1(out))\n",
    "        \n",
    "        return self.linear2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "074d404f-2940-453b-ad05-f402f17e425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(      \n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            Swish())\n",
    "        \n",
    "        self.conv2 = nn.Sequential( \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            Swish())\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "            \n",
    "        self.linear = nn.Linear(time_dim, out_channels)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out += self.linear(t)[:, :, None, None]\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        \n",
    "        return out + self.shortcut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba5930f2-f9af-486e-8137-64dc3516fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAttention(nn.Module):\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None):\n",
    "\n",
    "        \n",
    "        super(ImageAttention, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k     = d_k if d_k is not None else n_channels\n",
    "        self.scale   = self.d_k ** -0.5\n",
    "\n",
    "        self.linear_layer = nn.Linear(n_channels, n_heads * self.d_k * 3)\n",
    "        self.output_layer = nn.Linear(n_heads * self.d_k, n_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
    "\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        \n",
    "        _ = t\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        \n",
    "        QKV = self.linear_layer(x)\n",
    "        QKV = QKV.view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        \n",
    "        Q, K, V = torch.chunk(QKV, 3, dim=-1)\n",
    "        \n",
    "        att = (torch.einsum('BIHD, BJHD -> BIJH', Q, K) * self.scale).softmax(dim=1)\n",
    "        \n",
    "        out = torch.einsum('BIJH, BJHD -> BIHD', att, V)\n",
    "        out = out.reshape(batch_size, -1, self.n_heads*self.d_k)\n",
    "        out = self.output_layer(out)\n",
    "        out += x\n",
    "        \n",
    "        return out.permute(0, 2, 1).view(batch_size, n_channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "366e132c-e8e3-49fd-b8ab-cffb2a7c4fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, time_dim, att):\n",
    "        super(UNetEncoder, self).__init__()\n",
    "        \n",
    "        self.conv = DoubleConv(in_channels, out_channels, time_dim)        \n",
    "        self.att  = ImageAttention(out_channels) if att else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        return self.att(self.conv(x, t))\n",
    "\n",
    "class UNetBottleneck(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, time_dim):\n",
    "        super(UNetBottleneck, self).__init__()\n",
    "        \n",
    "        self.conv1 = DoubleConv(in_channels, in_channels, time_dim) \n",
    "        self.conv2 = DoubleConv(in_channels, in_channels, time_dim) \n",
    "        self.att   = ImageAttention(in_channels)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        out = self.att(self.conv1(x, t))\n",
    "        return self.conv2(out, t)\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, time_dim, att):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        \n",
    "        self.conv = DoubleConv(in_channels + out_channels, out_channels, time_dim)        \n",
    "        self.att  = ImageAttention(out_channels) if att else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        return self.att(self.conv(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d1916fb-6371-4c7e-b91d-82a61c71c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale down the feature map by $\\frac{1}{2} \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f23049e1-5cc6-4994-a333-2d1b879585b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale up the feature map by $2 \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0051c467-7649-425c-b619-166adfc46c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
    "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "                 n_blocks: int = 2):\n",
    "       \n",
    "        super().__init__()\n",
    "        n_resolutions = len(ch_mults)\n",
    "        \n",
    "        self.down = nn.ModuleList()\n",
    "        self.up   = nn.ModuleList()\n",
    "        \n",
    "        out_channels = in_channels = n_channels\n",
    "        \n",
    "        self.image_transform = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.time_emb        = TimeEmbedding(n_channels * 4)\n",
    "        \n",
    "        for i in range(n_resolutions):\n",
    "            \n",
    "            out_channels = in_channels * ch_mults[i]            \n",
    "            for _ in range(n_blocks):\n",
    "                self.down.append(UNetEncoder(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            \n",
    "            if i < n_resolutions - 1:\n",
    "                self.down.append(Downsample(in_channels))\n",
    "                \n",
    "        self.middle = UNetBottleneck(out_channels, n_channels * 4)\n",
    "        \n",
    "        in_channels = out_channels\n",
    "        \n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            \n",
    "            out_channels = in_channels\n",
    "            \n",
    "            for _ in range(n_blocks):\n",
    "                self.up.append(UNetDecoder(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "                \n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            self.up.append(UNetDecoder(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            in_channels = out_channels\n",
    "            \n",
    "            \n",
    "            if i > 0:\n",
    "                self.up.append(Upsample(in_channels))\n",
    "                \n",
    "        self.norm = nn.BatchNorm2d(n_channels)\n",
    "        self.act = Swish()\n",
    "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Get time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_transform(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, t)\n",
    "\n",
    "        # Second half of U-Net\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x, t)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x, t)\n",
    "\n",
    "        # Final normalization and convolution\n",
    "        return self.final(self.act(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1dad839-8daf-4a1f-807c-de8a3c0dfad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 32, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see it in action on dummy data:\n",
    "\n",
    "# A dummy batch of 10 3-channel 32px images\n",
    "x = torch.randn(10, 3, 32, 32)\n",
    "\n",
    "# 't' - what timestep are we on\n",
    "t = torch.tensor([50], dtype=torch.long)\n",
    "\n",
    "# Define the unet model\n",
    "unet = UNet()\n",
    "\n",
    "# The foreward pass (takes both x and t)\n",
    "model_output = unet(x, t)\n",
    "\n",
    "# The output shape matches the input.\n",
    "model_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb91b0-7661-4ef4-a6a5-76a70a834735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
