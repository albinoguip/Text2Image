{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce138c49-6c2e-4056-a06b-c7c5ecaa91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusionContinuousTimes(nn.Module):\n",
    "    def __init__(self, *, noise_schedule, timesteps):\n",
    "        super().__init__()\n",
    "        if noise_schedule == 'linear':\n",
    "            self.log_snr = beta_linear_log_snr\n",
    "        elif noise_schedule == \"cosine\":\n",
    "            self.log_snr = alpha_cosine_log_snr\n",
    "        else:\n",
    "            raise ValueError(f'invalid noise schedule {noise_schedule}')\n",
    "\n",
    "        self.num_timesteps = timesteps\n",
    "\n",
    "    def get_times(self, batch_size, noise_level, *, device):\n",
    "        return torch.full((batch_size,), noise_level, device = device, dtype = torch.long)\n",
    "\n",
    "    def sample_random_times(self, batch_size, max_thres = 0.999, *, device):\n",
    "        return torch.zeros((batch_size,), device = device).float().uniform_(0, max_thres)\n",
    "\n",
    "    def get_condition(self, times):\n",
    "        return maybe(self.log_snr)(times)\n",
    "\n",
    "    def get_sampling_timesteps(self, batch, *, device):\n",
    "        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n",
    "        times = repeat(times, 't -> b t', b = batch)\n",
    "        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n",
    "        times = times.unbind(dim = -1)\n",
    "        return times\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n",
    "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n",
    "\n",
    "        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr_next = self.log_snr(t_next)\n",
    "        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n",
    "\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
    "        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n",
    "\n",
    "        # c - as defined near eq 33\n",
    "        c = -expm1(log_snr - log_snr_next)\n",
    "        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n",
    "\n",
    "        # following (eq. 33)\n",
    "        posterior_variance = (sigma_next ** 2) * c\n",
    "        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def q_sample(self, x_start, t, noise = None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n",
    "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
    "        return alpha * x_start + sigma * noise, log_snr\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr = right_pad_dims_to(x_t, log_snr)\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
    "        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd325200-766d-484e-9eeb-022d39800fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
