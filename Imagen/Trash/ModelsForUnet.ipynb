{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3b60829a-5791-42ea-9821-718585d658e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops_exts.torch import EinopsToAndFrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "da242630-4e0e-4e73-acc2-ea8f651c4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionTypes(nn.Module):\n",
    "    def __init__(self, dim, dim_head = 64, heads = 8, dropout = 0.05, context_dim = None, norm_context = False, att_type='normal'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.scale         = dim_head ** -0.5\n",
    "        self.heads         = heads\n",
    "        self.att_type      = att_type\n",
    "        self.norm_context  = norm_context\n",
    "        # Normal Attention Initialization #############################################################################################################\n",
    "        \n",
    "        if att_type == 'normal':     \n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "            self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
    "            self.Q       = nn.Linear(dim, dim_head * heads, bias = False)\n",
    "            self.KV      = nn.Linear(dim, dim_head * 2, bias = False)\n",
    "\n",
    "            self.context      = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if context_dim is not None else None\n",
    "            self.output_layer = nn.Sequential(nn.Linear(dim_head * heads, dim, bias = False), nn.LayerNorm(dim))\n",
    "            \n",
    "        # Linear Attention Initialization #############################################################################################################\n",
    "            \n",
    "        elif att_type == 'linear':\n",
    "            \n",
    "            self.g1 = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "            \n",
    "            self.Q = nn.Sequential(nn.Dropout(dropout),\n",
    "                                   nn.Conv2d(dim, dim_head*heads, 1, bias=False),\n",
    "                                   nn.Conv2d(dim_head*heads, dim_head*heads, 3, bias=False, padding=1, groups=dim_head*heads))\n",
    "\n",
    "            self.K = nn.Sequential(nn.Dropout(dropout),\n",
    "                                   nn.Conv2d(dim, dim_head*heads, 1, bias=False),\n",
    "                                   nn.Conv2d(dim_head*heads, dim_head*heads, 3, bias=False, padding=1, groups=dim_head*heads))\n",
    "\n",
    "            self.V = nn.Sequential(nn.Dropout(dropout),\n",
    "                                   nn.Conv2d(dim, dim_head*heads, 1, bias=False),\n",
    "                                   nn.Conv2d(dim_head*heads, dim_head*heads, 3, bias=False, padding=1, groups=dim_head*heads))\n",
    "            \n",
    "            self.context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head*heads*2, bias=False)) if context_dim is not None else None\n",
    "\n",
    "            self.activation   = Swish()\n",
    "            self.output_layer = nn.Conv2d(dim_head*heads, dim, 1, bias=False)            \n",
    "            self.g2           = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "            \n",
    "        # Cross and Linear Cross Attention Initialization #############################################################################################\n",
    "\n",
    "        elif att_type == 'cross' or att_type == 'linear-cross':\n",
    "\n",
    "            context_dim = context_dim if context_dim is not None else dim\n",
    "\n",
    "            self.norm         = nn.LayerNorm(dim)\n",
    "            if norm_context:\n",
    "                self.norm_context = nn.LayerNorm(context_dim)\n",
    "\n",
    "            self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
    "            self.Q       = nn.Linear(dim, dim_head*heads, bias=False)\n",
    "            self.KV      = nn.Linear(context_dim, dim_head*heads*2, bias=False)\n",
    "\n",
    "            self.output_layer = nn.Sequential(nn.Linear(dim_head*heads, dim, bias=False), nn.LayerNorm(dim))\n",
    "    \n",
    "    #===================================================================================#\n",
    "    #                               ATTENTION FUNCTION                                  #\n",
    "    #===================================================================================#    \n",
    "        \n",
    "    def attention(self, q, k, v, mask = None, att_bias = None):\n",
    "        \n",
    "        # Normal Attention ######################################################\n",
    "        \n",
    "        if self.att_type == 'normal':\n",
    "            score = einsum('b h i d, b j d -> b h i j', q, k)\n",
    "\n",
    "            if att_bias is not None: score = score + att_bias\n",
    "\n",
    "            if mask is not None:            \n",
    "                max_neg = -torch.finfo(score.dtype).max            \n",
    "                mask    = mask[:, None, None, :]\n",
    "                score   = score.masked_fill(~mask, max_neg)\n",
    "\n",
    "            probs = score.softmax(dim = -1, dtype = torch.float32)\n",
    "            \n",
    "            print('probs', probs.shape)\n",
    "            print('v', v.shape)\n",
    "            \n",
    "            att = einsum('b h i j, b j d -> b h i d', probs, v)\n",
    "            att = rearrange(att, 'b h n d -> b n (h d)')\n",
    "            \n",
    "        # Linear Attention ######################################################\n",
    "            \n",
    "        elif self.att_type == 'linear':\n",
    "            \n",
    "            att = torch.matmul(k.transpose(-1, -2), v)\n",
    "            att = torch.matmul(q, att)\n",
    "            \n",
    "        # Cross Attention #######################################################\n",
    "            \n",
    "        elif self.att_type == 'cross':\n",
    "            score = torch.matmul(q, k.transpose(-1, -2))\n",
    "\n",
    "            if mask is not None:            \n",
    "                max_neg = -torch.finfo(score.dtype).max            \n",
    "                mask    = mask[:, None, None, :]\n",
    "                score   = score.masked_fill(~mask, max_neg)\n",
    "\n",
    "            probs = score.softmax(dim = -1, dtype = torch.float32)\n",
    "\n",
    "            att = torch.matmul(probs, v)\n",
    "            att = rearrange(att, 'b h n d -> b n (h d)')\n",
    "            \n",
    "        # Linear Cross Attention ################################################\n",
    "            \n",
    "        elif self.att_type == 'linear-cross':           \n",
    "\n",
    "            if mask is not None: \n",
    "                max_neg = -torch.finfo(score.dtype).max\n",
    "                mask = mask[:, :, None]\n",
    "                k, v = k.masked_fill(~mask, max_neg), v.masked_fill(~mask, 0.)\n",
    "\n",
    "            q, k = q.softmax(dim = -1)*self.scale, k.softmax(dim = -2)\n",
    "\n",
    "            att = torch.matmul(k.transpose(-1, -2), v)\n",
    "            att = torch.matmul(q, att)\n",
    "            att = rearrange(att, '(b h) n d -> b n (h d)', h = self.heads)\n",
    "        \n",
    "        return att\n",
    "    \n",
    "    #===================================================================================#\n",
    "    #                                      FORWARD                                      #\n",
    "    #===================================================================================#      \n",
    "\n",
    "    def forward(self, x, context = None, mask = None, att_bias = None):\n",
    "        \n",
    "        # Normal Attention #####################################################################################\n",
    "        \n",
    "        if self.att_type == 'normal': \n",
    "        \n",
    "            b, n, device = x.shape[0], x.shape[1], x.device\n",
    "\n",
    "            x = self.norm(x)\n",
    "\n",
    "            q = self.Q(x)\n",
    "            print(q.shape)\n",
    "            q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads) * self.scale\n",
    "            \n",
    "            k, v = self.KV(x).chunk(2, dim = -1)\n",
    "\n",
    "            nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n",
    "            k, v = torch.cat((nk, k), dim = -2), torch.cat((nv, v), dim = -2)\n",
    "\n",
    "            if context is not None:\n",
    "                assert self.context is not None\n",
    "                ck, cv = self.context(context).chunk(2, dim = -1)\n",
    "                k, v = torch.cat((ck, k), dim = -2), torch.cat((cv, v), dim = -2)\n",
    "\n",
    "            # ATTENTION\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = F.pad(mask, (1, 0), value = True)\n",
    "                att = self.attention(q=q, k=k, v=v, mask=mask, att_bias=att_bias)\n",
    "            else:\n",
    "                att = self.attention(q=q, k=k, v=v, att_bias=att_bias)\n",
    "                \n",
    "            out = self.output_layer(att) \n",
    "            \n",
    "        # Linear Attention #####################################################################################\n",
    "        \n",
    "        elif self.att_type == 'linear': \n",
    "\n",
    "            x1, y1, h = x.shape[-2], x.shape[-1], self.heads\n",
    "            \n",
    "            var  = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
    "            mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "            x    = (x - mean) / (var + 1e-15).sqrt() * self.g1   \n",
    "            \n",
    "            q, k, v = self.Q(x), self.K(x), self.V(x)\n",
    "            q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n",
    "\n",
    "            if context is not None:\n",
    "                assert self.context is not None\n",
    "                ck, cv = self.context(context).chunk(2, dim = -1)\n",
    "                ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n",
    "                k, v = torch.cat((k, ck), dim = -2), torch.cat((v, cv), dim = -2)\n",
    "\n",
    "            q, k = q.softmax(dim = -1)*self.scale, k.softmax(dim = -2)\n",
    "            \n",
    "            att = self.attention(q, k, v)\n",
    "\n",
    "            out = rearrange(att, '(b h) (x y) d -> b (h d) x y', h = h, x = x1, y = y1)\n",
    "            out = self.activation(out)\n",
    "            out = self.output_layer(out)\n",
    "            \n",
    "            var_out  = torch.var(out, dim = 1, unbiased = False, keepdim = True)\n",
    "            mean_out = torch.mean(out, dim = 1, keepdim = True)\n",
    "            out      = (x - mean_out) / (var_out + 1e-15).sqrt() * self.g2  \n",
    "            \n",
    "        # Cross and Linear Cross Attention #####################################################################\n",
    "            \n",
    "        elif self.att_type == 'cross' or self.att_type == 'linear-cross':\n",
    "        # elif self.att_type == 'cross':\n",
    "            \n",
    "            b, n, device = x.shape[0], x.shape[1], x.device\n",
    "\n",
    "            x       = self.norm(x)\n",
    "            context = self.norm_context(context) if self.norm_context else context\n",
    "\n",
    "            q    = self.Q(x)\n",
    "            k, v = self.KV(context).chunk(2, dim = -1)\n",
    "\n",
    "            if self.att_type == 'cross':    \n",
    "                \n",
    "                q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n",
    "                nk, nv  = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n",
    "                k, v, q = torch.cat((nk, k), dim = -2), torch.cat((nv, v), dim = -2), q*self.scale\n",
    "                \n",
    "            else:\n",
    "                q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n",
    "                nk, nv  = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n",
    "                k, v    = torch.cat((nk, k), dim = -2), torch.cat((nv, v), dim = -2)\n",
    "            \n",
    "            if mask is not None:\n",
    "                mask = F.pad(mask, (1, 0), value = True)\n",
    "                att = self.attention(q=q, k=k, v=v, mask=mask)\n",
    "            else:\n",
    "                att = self.attention(q=q, k=k, v=v)\n",
    "\n",
    "            out = self.output_layer(att)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860019d-1614-42cd-8a25-29f9db558097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEmbedLayer(nn.Module):\n",
    "    def __init__(self, dim_in, kernel_sizes, dim_out = None, stride = 2):\n",
    "        super().__init__()\n",
    "        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n",
    "        dim_out = default(dim_out, dim_in)\n",
    "\n",
    "        kernel_sizes = sorted(kernel_sizes)\n",
    "        num_scales = len(kernel_sizes)\n",
    "\n",
    "        # calculate the dimension at each scale\n",
    "        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n",
    "        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n",
    "\n",
    "        self.convs = nn.ModuleList([])\n",
    "        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n",
    "            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n",
    "        return torch.cat(fmaps, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9111be-953d-459c-beb8-eeb7569e6235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_sizes = [2, 4]\n",
    "stride = 2\n",
    "\n",
    "all([ks % 2 == stride % 2 for ks in kernel_sizes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fac28e2-cdf0-4429-9f76-b8fe1ed93810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride, kernel_sizes):\n",
    "        \n",
    "        super(CrossEmbedding, self).__init__()        \n",
    "        assert all([ks % 2 == stride % 2 for ks in kernel_sizes]), 'Kernel and Stride must be odd or even, both'\n",
    "        assert all([ks >= stride for ks in kernel_sizes]), 'All Kernels must be larger than Stride'\n",
    "        \n",
    "        kernel_sizes, n_kernels = sorted(kernel_sizes), len(kernel_sizes)\n",
    "        \n",
    "        dim_scales = [int(out_channels / (2 ** i)) for i in range(1, n_kernels)]\n",
    "        dim_scales.append(out_channels - sum(dim_scales))\n",
    "        \n",
    "        self.conv_layer = nn.ModuleList([nn.Conv2d(in_channels, dim_scale, kernel,\n",
    "                                                   stride = stride, padding = (kernel - stride) // 2)\n",
    "                                        for kernel, dim_scale in zip(kernel_sizes, dim_scales)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = [conv(x) for conv in self.conv_layer]\n",
    "        return torch.cat(out, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d65e9f13-f714-47d2-a70e-9358a03fa821",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "All Kernels must be larger than Stride",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ce \u001b[38;5;241m=\u001b[39m \u001b[43mCrossEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m ce\n",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36mCrossEmbedding.__init__\u001b[1;34m(self, in_channels, out_channels, stride, kernel_sizes)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28msuper\u001b[39m(CrossEmbedding, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()        \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m([ks \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m stride \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ks \u001b[38;5;129;01min\u001b[39;00m kernel_sizes]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKernel and Stride must be odd or even, both\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m([ks \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m stride \u001b[38;5;28;01mfor\u001b[39;00m ks \u001b[38;5;129;01min\u001b[39;00m kernel_sizes]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll Kernels must be larger than Stride\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m kernel_sizes, n_kernels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(kernel_sizes), \u001b[38;5;28mlen\u001b[39m(kernel_sizes)\n\u001b[0;32m     11\u001b[0m dim_scales \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(out_channels \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m i)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_kernels)]\n",
      "\u001b[1;31mAssertionError\u001b[0m: All Kernels must be larger than Stride"
     ]
    }
   ],
   "source": [
    "ce = CrossEmbedding(in_channels=3, out_channels=3, stride=4, kernel_sizes=[2, 4])\n",
    "ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d91e312f-5f68-4a0b-b917-6ca34d54b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 10, 10))\n",
    "x1, x2 = ce(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9ca00f4-a733-4368-93c3-051e173b4864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1607,  0.0354,  0.0207,  0.3303, -0.0195],\n",
       "          [ 0.3290, -0.0417,  0.4365, -0.1227, -0.0551],\n",
       "          [-0.1933,  0.1280, -0.1405, -0.1524, -0.1422],\n",
       "          [ 0.1780, -0.0020, -0.2729,  0.0030, -0.0936],\n",
       "          [ 0.2232,  0.0090, -0.0836,  0.2321,  0.0535]],\n",
       "\n",
       "         [[-0.0992, -0.0362,  0.0603, -0.1646, -0.2857],\n",
       "          [-0.2236,  0.1094,  0.0615, -0.0907, -0.0036],\n",
       "          [ 0.0175, -0.0200, -0.1888,  0.0071,  0.0780],\n",
       "          [ 0.2778,  0.1466,  0.0529,  0.0920,  0.1258],\n",
       "          [ 0.3529,  0.2027,  0.0930,  0.1575,  0.2072]],\n",
       "\n",
       "         [[-0.3897, -0.7092, -0.3089, -0.3905, -0.5074],\n",
       "          [-0.4342,  0.0895,  0.0833, -0.0590,  0.0147],\n",
       "          [-0.5677, -0.3688, -0.0239, -0.2113, -0.1807],\n",
       "          [-0.2386, -0.0758, -0.1090, -0.1430, -0.0996],\n",
       "          [ 0.2304,  0.3404,  0.1393, -0.0335,  0.0495]]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b75cf08-0565-4e3e-874f-307a3bc380a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1607,  0.0354,  0.0207,  0.3303, -0.0195],\n",
       "          [ 0.3290, -0.0417,  0.4365, -0.1227, -0.0551],\n",
       "          [-0.1933,  0.1280, -0.1405, -0.1524, -0.1422],\n",
       "          [ 0.1780, -0.0020, -0.2729,  0.0030, -0.0936],\n",
       "          [ 0.2232,  0.0090, -0.0836,  0.2321,  0.0535]],\n",
       "\n",
       "         [[-0.0992, -0.0362,  0.0603, -0.1646, -0.2857],\n",
       "          [-0.2236,  0.1094,  0.0615, -0.0907, -0.0036],\n",
       "          [ 0.0175, -0.0200, -0.1888,  0.0071,  0.0780],\n",
       "          [ 0.2778,  0.1466,  0.0529,  0.0920,  0.1258],\n",
       "          [ 0.3529,  0.2027,  0.0930,  0.1575,  0.2072]],\n",
       "\n",
       "         [[-0.3897, -0.7092, -0.3089, -0.3905, -0.5074],\n",
       "          [-0.4342,  0.0895,  0.0833, -0.0590,  0.0147],\n",
       "          [-0.5677, -0.3688, -0.0239, -0.2113, -0.1807],\n",
       "          [-0.2386, -0.0758, -0.1090, -0.1430, -0.0996],\n",
       "          [ 0.2304,  0.3404,  0.1393, -0.0335,  0.0495]]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfdb836d-d0ba-4cc5-8fa3-75e7d68211f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_scales = [i for i in range(1,3)]\n",
    "dim_scales = [*dim_scales, 4]\n",
    "dim_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c877ad89-deaa-44f3-a317-17df777d2ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_scales = [i for i in range(1,3)]\n",
    "dim_scales.append(4)\n",
    "dim_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ce9d0f70-4c6a-46ce-b0c9-daae6a15d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Always():\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b557882d-bc32-42fd-ba84-04cf9c91d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5c01136a-2421-46b4-ad57-b0ffdb4698cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07da6be4-5de9-43a8-a75f-4058fd1a5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        context_dim = None,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        norm_context = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        context_dim = context_dim if context_dim is not None else dim\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if norm_context else Identity()\n",
    "\n",
    "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias = False),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context, mask = None):\n",
    "        b, n, device = *x.shape[:2], x.device\n",
    "\n",
    "        x = self.norm(x)\n",
    "        context = self.norm_context(context)\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n",
    "\n",
    "        # add null key / value for classifier free guidance in prior net\n",
    "\n",
    "        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n",
    "\n",
    "        k = torch.cat((nk, k), dim = -2)\n",
    "        v = torch.cat((nv, v), dim = -2)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "        max_neg_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = F.pad(mask, (1, 0), value = True)\n",
    "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
    "            sim = sim.masked_fill(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cf09f111-c921-4d85-a9b5-49096e59f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebbcb3cb-aebe-47da-a987-5f19717c26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups = 8):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(groups, dim)\n",
    "        self.acti = Swish()\n",
    "        self.conv = nn.Conv2d(dim, dim_out, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.activation(x)\n",
    "        return self.project(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f6b25a64-1ee0-424c-92b4-f49cb4c9ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalContext(nn.Module):\n",
    "    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.to_k = nn.Conv2d(dim_in, 1, 1)\n",
    "        hidden_dim = max(3, dim_out // 2)\n",
    "\n",
    "        self.net = nn.Sequential(nn.Conv2d(dim_in, hidden_dim, 1),\n",
    "                                 nn.SiLU(),\n",
    "                                 nn.Conv2d(hidden_dim, dim_out, 1),\n",
    "                                 nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = self.to_k(x)\n",
    "        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n",
    "        print(context.shape)\n",
    "        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n",
    "        out = rearrange(out, '... -> ... 1')\n",
    "        print(out.shape)\n",
    "        return self.net(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "190f1f3e-59ae-4b22-b73d-ba1274f2e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_out,\n",
    "        *,\n",
    "        cond_dim = None,\n",
    "        time_cond_dim = None,\n",
    "        groups = 8,\n",
    "        linear_attn = False,\n",
    "        use_gca = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_mlp = None\n",
    "\n",
    "        if exists(time_cond_dim):\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_cond_dim, dim_out * 2)\n",
    "            )\n",
    "\n",
    "        self.cross_attn = None\n",
    "\n",
    "        if exists(cond_dim):\n",
    "            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n",
    "\n",
    "            self.cross_attn = EinopsToAndFrom(\n",
    "                'b c h w',\n",
    "                'b (h w) c',\n",
    "                attn_klass(\n",
    "                    dim = dim_out,\n",
    "                    context_dim = cond_dim\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups = groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups = groups)\n",
    "\n",
    "        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n",
    "\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x, time_emb = None, cond = None):\n",
    "\n",
    "        scale_shift = None\n",
    "        if exists(self.time_mlp) and exists(time_emb):\n",
    "            time_emb = self.time_mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
    "            scale_shift = time_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x)\n",
    "\n",
    "        if exists(self.cross_attn):\n",
    "            assert exists(cond)\n",
    "            h = self.cross_attn(h, context = cond) + h\n",
    "\n",
    "        print(h.shape)\n",
    "            \n",
    "        h = self.block2(h, scale_shift = scale_shift)\n",
    "\n",
    "        h = h * self.gca(h)\n",
    "\n",
    "        return h + self.res_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b76985f3-cb5c-43cd-a438-7a08f81ecfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, cond_dim = None, time_cond_dim = None,\n",
    "                 groups = 8, linear_att = False, use_gca = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_gca = use_gca\n",
    "\n",
    "        self.time_NN = nn.Sequential(Swish(), nn.Linear(time_cond_dim, dim_out*2))\n",
    "\n",
    "        self.att_layer = None\n",
    "        if cond_dim is not None:\n",
    "            att_type = 'cross' if not linear_att else 'linear-cross'    \n",
    "            self.att_layer = AttentionTypes(dim = dim_out, context_dim = cond_dim, att_type=att_type)\n",
    "\n",
    "        self.seq1 = nn.Sequential(nn.GroupNorm(groups, dim),\n",
    "                                  Swish(),\n",
    "                                  nn.Conv2d(dim, dim_out, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.seq2 = nn.Sequential(Swish(),\n",
    "                                  nn.Conv2d(dim_out, dim_out, kernel_size=3, padding=1))\n",
    "        \n",
    "        if use_gca:\n",
    "            self.conv = nn.Conv2d(dim_out, 1, 1)\n",
    "            self.seq3 = nn.Sequential(nn.Conv2d(dim_out, max(3, dim_out//2), 1),\n",
    "                                      nn.SiLU(),\n",
    "                                      nn.Conv2d(max(3, dim_out//2), dim_out, 1),\n",
    "                                      nn.Sigmoid())\n",
    "\n",
    "        self.output_layer = None\n",
    "        if dim != dim_out:\n",
    "            self.output_layer = nn.Conv2d(dim, dim_out, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, time_emb = None, cond = None):\n",
    "\n",
    "        scale, shift = None, None\n",
    "        if self.time_NN is not None and time_emb is not None:\n",
    "            \n",
    "            time_emb = self.time_NN(time_emb)\n",
    "            scale, shift = time_emb[:, :, None, None].chunk(2, dim = 1)\n",
    "\n",
    "        h = self.seq1(x)\n",
    "\n",
    "        if self.att_layer is not None:\n",
    "            assert cond is not None\n",
    "            \n",
    "            w = h.shape[-1]            \n",
    "            m = rearrange(h, 'b c h w -> b (h w) c')\n",
    "            m = self.att_layer(m, context = cond) \n",
    "            h = rearrange(m, 'b (h w) c -> b c h w',w=w) + h                    \n",
    "                \n",
    "        print(h.shape)\n",
    "                \n",
    "        h = self.norm(h)*(scale + 1) + shift if scale is not None else self.norm(h)        \n",
    "        h = self.seq2(h)\n",
    "        \n",
    "        if self.use_gca:\n",
    "            c    = self.conv(h)\n",
    "            y, c = rearrange_many((h, c), 'b n ... -> b n (...)')\n",
    "\n",
    "            c    = c.softmax(dim = -1) \n",
    "            print(c.shape)\n",
    "            o = torch.matmul(y, c.transpose(1, 2))[:, :, :, None]\n",
    "            \n",
    "            print(o.shape)\n",
    "        \n",
    "            h = h*self.seq3(o)\n",
    "        \n",
    "        return h + self.output_layer(x) if self.output_layer is not None else h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "301063ee-9b4a-4040-a98b-be256c6b78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = ResnetBlock(dim = 64, dim_out=16, cond_dim = 4, time_cond_dim = 4,\n",
    "                 groups = 8, linear_att = True, use_gca = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9260af81-f6af-49ee-a0cb-39d0e69064dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 1, 400])\n",
      "torch.Size([1, 16, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 20, 20])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((1, 64, 20, 20))\n",
    "t = torch.rand((1, 4)) \n",
    "c = torch.rand((1, 16, 4)) \n",
    "\n",
    "rb(x=x, time_emb = t, cond = c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "90eba6f9-09ad-4384-ab50-c70057f56b01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearCrossAttention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [138]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rb \u001b[38;5;241m=\u001b[39m \u001b[43mResnetBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_cond_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_attn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gca\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [137]\u001b[0m, in \u001b[0;36mResnetBlock.__init__\u001b[1;34m(self, dim, dim_out, cond_dim, time_cond_dim, groups, linear_attn, use_gca)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(cond_dim):\n\u001b[1;32m---> 26\u001b[0m     attn_klass \u001b[38;5;241m=\u001b[39m CrossAttention \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m linear_attn \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mLinearCrossAttention\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn \u001b[38;5;241m=\u001b[39m EinopsToAndFrom(\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c h w\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb (h w) c\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m         )\n\u001b[0;32m     35\u001b[0m     )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1 \u001b[38;5;241m=\u001b[39m Block(dim, dim_out, groups \u001b[38;5;241m=\u001b[39m groups)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LinearCrossAttention' is not defined"
     ]
    }
   ],
   "source": [
    "rb = ResnetBlock(dim = 64, dim_out=16, cond_dim = 4, time_cond_dim = 4,\n",
    "                 groups = 8, linear_attn = True, use_gca = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "eb16e0c0-f54e-400b-8e76-a99e98f277b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 1, 400])\n",
      "torch.Size([1, 16, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 20, 20])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((1, 64, 20, 20))\n",
    "t = torch.rand((1, 4)) \n",
    "c = torch.rand((1, 16, 4)) \n",
    "\n",
    "rb(x=x, time_emb = t, cond = c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a670b7f8-483b-4712-894f-5b003a8cb972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.rand((1, 2, 3))\n",
    "r = torch.rand((1, 4, 3))\n",
    "\n",
    "einsum('b i n, b c n -> b c i', v, r).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "60e5b18d-1601-4c83-aeba-22e732c89c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrange(v, '... -> ... 1').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3eab20b5-9744-4721-98fc-f24aca417d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, :, :, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f577921-a5e2-489d-a179-b5d722b93c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(v, r.transpose(1, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f83cb084-e94c-4f09-b39f-d48d97e7dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = ResnetBlock(dim = 64, dim_out=16, cond_dim = 4, time_cond_dim = 4,\n",
    "                 groups = 8, linear_att = False, use_gca = False, squeeze_excite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d2bbeed5-a980-4354-8bad-d696c00b447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400, 16])\n",
      "torch.Size([1, 400, 16])\n",
      "torch.Size([1, 16, 20, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 20, 20])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((1, 64, 20, 20))\n",
    "t = torch.rand((1, 4)) \n",
    "c = torch.rand((1, 16, 4)) \n",
    "\n",
    "rb(x=x, time_emb = t, cond = c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "82ea501a-5e38-4fc4-b763-a6ff9bdc4d2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [64] and input of shape [1, 16, 20, 20]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)) \n\u001b[0;32m      3\u001b[0m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m4\u001b[39m)) \n\u001b[1;32m----> 5\u001b[0m \u001b[43mrb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_emb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ENV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[1;34m(self, x, time_emb, cond)\u001b[0m\n\u001b[0;32m     53\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_layer(m, context \u001b[38;5;241m=\u001b[39m cond) \n\u001b[0;32m     54\u001b[0m     h \u001b[38;5;241m=\u001b[39m rearrange(m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb (h w) c -> b c h w\u001b[39m\u001b[38;5;124m'\u001b[39m,w\u001b[38;5;241m=\u001b[39mw) \u001b[38;5;241m+\u001b[39m h                    \n\u001b[1;32m---> 56\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m(scale \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m shift \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(h)        \n\u001b[0;32m     57\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq2(h)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gca:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ENV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ENV\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:268\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ENV\\lib\\site-packages\\torch\\nn\\functional.py:2360\u001b[0m, in \u001b[0;36mgroup_norm\u001b[1;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(group_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias,), \u001b[38;5;28minput\u001b[39m, num_groups, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps)\n\u001b[0;32m   2359\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[1;32m-> 2360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [64] and input of shape [1, 16, 20, 20]"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1, 64, 20, 20))\n",
    "t = torch.rand((1, 4)) \n",
    "c = torch.rand((1, 16, 4)) \n",
    "\n",
    "rb(x=x, time_emb = t, cond = c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38003ca0-a52b-462c-8f96-42d5ee46fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from einops_exts import rearrange_many, repeat_many\n",
    "from torch import nn, einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f6e0143-0b35-4d2a-b7a8-3a0afae20ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResnetBlock(\n",
       "  (time_NN): Sequential(\n",
       "    (0): Swish()\n",
       "    (1): Linear(in_features=4, out_features=32, bias=True)\n",
       "  )\n",
       "  (cross_attn): CrossAttention(\n",
       "    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_context): Identity()\n",
       "    (to_q): Linear(in_features=16, out_features=512, bias=False)\n",
       "    (to_kv): Linear(in_features=4, out_features=1024, bias=False)\n",
       "    (to_out): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=16, bias=False)\n",
       "      (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (block1): Block(\n",
       "    (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "    (activation): SiLU()\n",
       "    (project): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (block2): Block(\n",
       "    (groupnorm): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "    (activation): SiLU()\n",
       "    (project): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (res_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb = ResnetBlock(dim = 64, dim_out=16, cond_dim = 4, time_cond_dim = 4,\n",
    "            groups = 8, linear_attn = False, use_gca = False, squeeze_excite = False)\n",
    "\n",
    "rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "696e46f8-fa06-4364-b1e4-f998dd6781be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        context_dim = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n",
    "\n",
    "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias = False),\n",
    "            LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, attn_bias = None):\n",
    "        b, n, device = *x.shape[:2], x.device\n",
    "\n",
    "        x = self.norm(x)\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n",
    "        print('v', v.shape)\n",
    "        \n",
    "        print('q', q.shape)\n",
    "\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n",
    "        q = q * self.scale\n",
    "\n",
    "        # add null key / value for classifier free guidance in prior net\n",
    "\n",
    "        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n",
    "        k = torch.cat((nk, k), dim = -2)\n",
    "        v = torch.cat((nv, v), dim = -2)\n",
    "        print('v', v.shape)\n",
    "\n",
    "        # add text conditioning, if present\n",
    "\n",
    "        if exists(context):\n",
    "            assert exists(self.to_context)\n",
    "            ck, cv = self.to_context(context).chunk(2, dim = -1)\n",
    "            k = torch.cat((ck, k), dim = -2)\n",
    "            v = torch.cat((cv, v), dim = -2)\n",
    "            print('v', v.shape)\n",
    "\n",
    "        # calculate query / key similarities\n",
    "\n",
    "        sim = einsum('b h i d, b j d -> b h i j', q, k)\n",
    "\n",
    "        # relative positional encoding (T5 style)\n",
    "\n",
    "        if exists(attn_bias):\n",
    "            sim = sim + attn_bias\n",
    "\n",
    "        # masking\n",
    "\n",
    "        max_neg_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = F.pad(mask, (1, 0), value = True)\n",
    "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
    "            sim = sim.masked_fill(~mask, max_neg_value)\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
    "\n",
    "        # aggregate values\n",
    "\n",
    "        print('attn', attn.shape)\n",
    "        print('v', v.shape)\n",
    "        \n",
    "        out = einsum('b h i j, b j d -> b h i d', attn, v)\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        print('out', out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "850b4fce-e5d4-412e-86fc-e4a6eb81817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_head = 32,\n",
    "        heads = 8,\n",
    "        dropout = 0.05,\n",
    "        context_dim = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "        self.norm = ChanLayerNorm(dim)\n",
    "\n",
    "        self.nonlin = nn.SiLU()\n",
    "\n",
    "        self.to_q = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
    "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
    "        )\n",
    "\n",
    "        self.to_k = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
    "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
    "        )\n",
    "\n",
    "        self.to_v = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(dim, inner_dim, 1, bias = False),\n",
    "            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
    "        )\n",
    "\n",
    "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(inner_dim, dim, 1, bias = False),\n",
    "            ChanLayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, fmap, context = None):\n",
    "        h, x, y = self.heads, *fmap.shape[-2:]\n",
    "\n",
    "        fmap = self.norm(fmap)\n",
    "        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n",
    "        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n",
    "\n",
    "        if exists(context):\n",
    "            assert exists(self.to_context)\n",
    "            ck, cv = self.to_context(context).chunk(2, dim = -1)\n",
    "            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n",
    "            k = torch.cat((k, ck), dim = -2)\n",
    "            v = torch.cat((v, cv), dim = -2)\n",
    "\n",
    "        q = q.softmax(dim = -1)\n",
    "        k = k.softmax(dim = -2)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        context = einsum('b n d, b n e -> b d e', k, v)\n",
    "        out = einsum('b n d, b d e -> b n e', q, context)\n",
    "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n",
    "\n",
    "        out = self.nonlin(out)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c249c-d46b-481d-b4d7-fb782252537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = rearrange(h, 'b c h w -> b (h w) c')\n",
    "m = self.att_layer(m, context = cond) \n",
    "h = rearrange(m, 'b (h w) c -> b c h w',w=w) + h\n",
    "\n",
    "self.cross_attn = EinopsToAndFrom(\n",
    "                'b c h w',\n",
    "                'b (h w) c',\n",
    "                attn_klass(\n",
    "                    dim = dim_out,\n",
    "                    context_dim = cond_dim\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "54ebbc9a-ce51-46b7-9237-f75f6a06505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChanLayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
    "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "        return (x - mean) / (var + self.eps).sqrt() * self.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ad553b19-a639-4ffd-85bc-21910b2bbf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 32, ff_mult = 2, context_dim = None):\n",
    "        super().__init__()\n",
    "        self.attn = EinopsToAndFrom('b c h w', 'b (h w) c',\n",
    "                                    Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim))\n",
    "        self.ff = ChanFeedForward(dim = dim, mult = ff_mult)\n",
    "\n",
    "    def forward(self, x, context = None):\n",
    "        x = self.attn(x, context = context) + x\n",
    "        x = self.ff(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "16594427-e1ad-48f0-abfe-69bc9703d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttentionTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 32, ff_mult = 2, context_dim = None):\n",
    "        super().__init__()\n",
    "        self.attn = LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim)\n",
    "        self.ff = ChanFeedForward(dim = dim, mult = ff_mult)\n",
    "\n",
    "    def forward(self, x, context = None):\n",
    "        x = self.attn(x, context = context) + x\n",
    "        x = self.ff(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a6c0b782-f71a-4c91-a3e5-a942cee80f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 32, ff_mult = 2, att_type='normal', context_dim = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.att_type = att_type\n",
    "        \n",
    "        self.att = AttentionTypes(dim=dim, heads=heads, dim_head=dim_head, context_dim=context_dim, att_type=att_type)\n",
    "        self.g1    = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.conv1 = nn.Conv2d(dim, dim*ff_mult, 1, bias = False)\n",
    "        self.activ = Swish()\n",
    "        self.g2    = nn.Parameter(torch.ones(1, dim*ff_mult, 1, 1))\n",
    "        self.conv2 = nn.Conv2d(dim*ff_mult, dim, 1, bias = False)\n",
    "\n",
    "    def forward(self, x, context = None):\n",
    "        \n",
    "        if self.att_type == 'normal':\n",
    "            w = x.shape[-1]    \n",
    "            x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "            x = self.att(x, context = context) + x\n",
    "            x = rearrange(x, 'b (h w) c -> b c h w',w=w)\n",
    "            \n",
    "        elif self.att_type == 'linear':\n",
    "            x = self.att(x, context = context) + x\n",
    "        \n",
    "        num = x - torch.mean(x, dim=1, keepdim=True)\n",
    "        den = (torch.var(x, dim=1, unbiased=False, keepdim=True) + 1e-5).sqrt()\n",
    "        x1  = (num/den)*self.g1\n",
    "        \n",
    "        x1 = self.activ(self.conv1(x1))\n",
    "        \n",
    "        num = x1 - torch.mean(x1, dim=1, keepdim=True)\n",
    "        den = (torch.var(x1, dim=1, unbiased=False, keepdim=True) + 1e-5).sqrt()\n",
    "        x1  = (num/den)*self.g2\n",
    "        \n",
    "        out = self.conv2(x1) + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2e2c31a0-268a-42ee-9b4c-5299f39df52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = LinearAttentionTransformerBlock(dim=4, heads = 8, dim_head = 32, ff_mult = 2, context_dim = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6177b84a-2b7b-47d0-add4-43f351743dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 15])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((1, 8, 15))\n",
    "li(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "c57c3954-329f-4956-9aab-837e697e5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TransformerBlock(dim=4, heads = 8, dim_head = 32, ff_mult = 2, context_dim = None, att_type='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f18e0195-a90c-4600-81b4-73715933cad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 15])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((1, 8, 15))\n",
    "tb(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "754358a0-6c43-42d9-84f6-56862861b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TransformerBlock(dim=4, heads = 8, dim_head = 32, ff_mult = 2, context_dim = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6fc1b4e9-0939-45e6-b488-76154225fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v torch.Size([1, 16, 32])\n",
      "q torch.Size([1, 16, 256])\n",
      "v torch.Size([1, 17, 32])\n",
      "attn torch.Size([1, 8, 16, 17])\n",
      "v torch.Size([1, 17, 32])\n",
      "out torch.Size([1, 16, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4, 4])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((1, 4, 4, 4))\n",
    "tb(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e502f8e9-08d8-4685-8348-d46cca3e2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TransformerBlock(dim=4, heads = 8, dim_head = 32, ff_mult = 2, context_dim = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ebaee347-72a7-448c-b184-f1958c77bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 256])\n",
      "probs torch.Size([1, 8, 16, 17])\n",
      "v torch.Size([1, 17, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4, 4])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((1, 4, 4, 4))\n",
    "tb(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1cfdecbc-86e2-4674-a018-29c36e76414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dfbe9880-1e51-428f-926e-04812d36f9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.0000)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x    = torch.tensor(4)\n",
    "mean = torch.tensor(2)\n",
    "var  = torch.tensor(9)\n",
    "g1   = torch.tensor(15)\n",
    "\n",
    "(x - mean)/(var + 1e-5).sqrt()*g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "12caaacb-b471-499a-8a1d-af99bd36d9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.0000)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = x - mean\n",
    "den = (var + 1e-5).sqrt()\n",
    "(num/den)*g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ab7b27c9-7d0d-412d-9ead-98f55697d32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.0000)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (var + 1e-5).sqrt()\n",
    "b = (x - mean)\n",
    "c = b/a\n",
    "c*g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "74ddc722-836b-4c75-8ef4-d64b6227ded0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChanFeedForward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [141]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tb \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [139]\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[1;34m(self, dim, heads, dim_head, ff_mult, context_dim)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt \u001b[38;5;241m=\u001b[39m AttentionTypes(dim \u001b[38;5;241m=\u001b[39m dim, heads \u001b[38;5;241m=\u001b[39m heads, dim_head \u001b[38;5;241m=\u001b[39m dim_head, context_dim \u001b[38;5;241m=\u001b[39m context_dim)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff \u001b[38;5;241m=\u001b[39m \u001b[43mChanFeedForward\u001b[49m(dim \u001b[38;5;241m=\u001b[39m dim, mult \u001b[38;5;241m=\u001b[39m ff_mult)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChanFeedForward' is not defined"
     ]
    }
   ],
   "source": [
    "tb = TransformerBlock(dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e4a0a-3391-46c1-8fd9-abb164ca9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n",
    "    hidden_dim = int(dim * mult)\n",
    "    return nn.Sequential(\n",
    "        ChanLayerNorm(dim),\n",
    "        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n",
    "        nn.GELU(),\n",
    "        ChanLayerNorm(hidden_dim),\n",
    "        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "14d1e143-9287-47b9-a062-604b41b31f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer('beta', torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2cec39d6-9454-4f86-a15e-03352ae102e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n",
    "    hidden_dim = int(dim * mult)\n",
    "    return nn.Sequential(\n",
    "        ChanLayerNorm(dim),\n",
    "        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n",
    "        nn.GELU(),\n",
    "        ChanLayerNorm(hidden_dim),\n",
    "        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fcad23-4973-41cf-8585-a3e679666c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Downsample(dim, dim_out = None):\n",
    "    dim_out = default(dim_out, dim)\n",
    "    return nn.Conv2d(dim, dim_out, 4, 2, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
