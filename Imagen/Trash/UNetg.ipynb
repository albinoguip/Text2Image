{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f592d3d0-a88a-41db-be26-272ad9e41af6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define model of double convolution\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDoubleConv\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels, out_channels):\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28msuper\u001b[39m(DoubleConv, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Define model of double convolution\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "        \n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e24dc26-7c91-47ae-ae07-2f2b2b7cc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff7c6a3-db8e-45a7-a00e-5bc2620a1b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model of double convolution\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "        \n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c81e88-d68c-425a-a4a0-0f15c8c5174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, features=[64, 128, 256, 512]):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool  = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "            \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.skip_connections = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            self.skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a1bd43-0699-4488-81db-856a7ac6beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(in_channels, features)\n",
    "        self.ups     = nn.ModuleList()\n",
    "\n",
    "        # self.fce     = EncoderFC(512)\n",
    "        # self.fcd     = DecoderFC(512)\n",
    "        \n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "            \n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.decoder_skip_connections = []\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        self.decoder_skip_connections.append(x)\n",
    "\n",
    "        \n",
    "        self.encoder_skip_connections = self.encoder.skip_connections[::-1]\n",
    "        \n",
    "        for i in range(0, len(self.ups), 2):\n",
    "            \n",
    "            x = self.ups[i](x)\n",
    "            \n",
    "            self.decoder_skip_connections.append(x)\n",
    "        \n",
    "            skip_connection = self.encoder_skip_connections[i//2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "                \n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            \n",
    "            x = self.ups[i+1](concat_skip)\n",
    "        \n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a7b540c-e18e-4d58-aec9-3480ecffd3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = UNet(3, 3, [64, 128, 128, 256])\n",
    "\n",
    "x = torch.rand((10, 3, 32, 32))\n",
    "u.encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3f577-9b81-4b63-b060-ac9ecc4b647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Unet Definition\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple, Union, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# A fancy activation function\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Swish actiavation function\n",
    "    $$x \\cdot \\sigma(x)$$\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "# The time embedding \n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Embeddings for $t$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of dimensions in the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        # First linear layer\n",
    "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
    "        # Activation\n",
    "        self.act = Swish()\n",
    "        # Second linear layer\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        # Create sinusoidal position embeddings\n",
    "        # [same as those from the transformer](../../transformers/positional_encoding.html)\n",
    "        #\n",
    "        # \\begin{align}\n",
    "        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n",
    "        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n",
    "        # \\end{align}\n",
    "        #\n",
    "        # where $d$ is `half_dim`\n",
    "        half_dim = self.n_channels // 8\n",
    "        emb = math.log(10_000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "\n",
    "        #\n",
    "        return emb\n",
    "\n",
    "# Residual blocks include 'skip' connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Residual block\n",
    "    A residual block has two convolution layers with group normalization.\n",
    "    Each resolution is processed with two residual blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `in_channels` is the number of input channels\n",
    "        * `out_channels` is the number of input channels\n",
    "        * `time_channels` is the number channels in the time step ($t$) embeddings\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Group normalization and the first convolution layer\n",
    "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "        self.act1 = Swish()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # Group normalization and the second convolution layer\n",
    "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        self.act2 = Swish()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        # Linear layer for time embeddings\n",
    "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # First convolution layer\n",
    "        h = self.conv1(self.act1(self.norm1(x)))\n",
    "        # Add time embeddings\n",
    "        h += self.time_emb(t)[:, :, None, None]\n",
    "        # Second convolution layer\n",
    "        h = self.conv2(self.act2(self.norm2(h)))\n",
    "\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "# Ahh yes, magical attention...\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Attention block\n",
    "    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of channels in the input\n",
    "        * `n_heads` is the number of heads in multi-head attention\n",
    "        * `d_k` is the number of dimensions in each head\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Default `d_k`\n",
    "        if d_k is None:\n",
    "            d_k = n_channels\n",
    "        # Normalization layer\n",
    "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
    "        # Projections for query, key and values\n",
    "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        # Linear layer for final transformation\n",
    "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
    "        # Scale for dot-product attention\n",
    "        self.scale = d_k ** -0.5\n",
    "        #\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        # Get shape\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
    "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
    "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = attn.softmax(dim=1)\n",
    "        # Multiply by values\n",
    "        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n",
    "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
    "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # Transform to `[batch_size, seq, n_channels]`\n",
    "        res = self.output(res)\n",
    "\n",
    "        # Add skip connection\n",
    "        res += x\n",
    "\n",
    "        # Change to shape `[batch_size, in_channels, height, width]`\n",
    "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "\n",
    "        #\n",
    "        return res\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Down block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Up block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Middle block\n",
    "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n",
    "    This block is applied at the lowest resolution of the U-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, time_channels: int):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "        self.attn = AttentionBlock(n_channels)\n",
    "        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res1(x, t)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x, t)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale up the feature map by $2 \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale down the feature map by $\\frac{1}{2} \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)\n",
    "\n",
    "# The core class definition (aka the important bit)\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ## U-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
    "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "                 n_blocks: int = 2):\n",
    "        \"\"\"\n",
    "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
    "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
    "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
    "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
    "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        # Project image into feature map\n",
    "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # Time embedding layer. Time embedding has `n_channels * 4` channels\n",
    "        self.time_emb = TimeEmbedding(n_channels * 4)\n",
    "\n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, n_channels * 4, )\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        # Final normalization and convolution layer\n",
    "        self.norm = nn.GroupNorm(8, n_channels)\n",
    "        self.act = Swish()\n",
    "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Get time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, t)\n",
    "\n",
    "        # Second half of U-Net\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x, t)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x, t)\n",
    "\n",
    "        # Final normalization and convolution\n",
    "        return self.final(self.act(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89b22a-d28e-4df4-a368-203bf33dafea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0f19e-127c-4d95-9ff9-797778effc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0bc6a-10c5-420c-9ecf-ef51ca5a31c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc82b4e-a010-45f4-a358-252d5f6d4501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47354f32-9162-48fb-b721-1b5d0ea5a584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b128bf5-1a6e-40e2-85f9-8613b51a8356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        image_embed_dim = 1024,\n",
    "        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n",
    "        num_resnet_blocks = 1,\n",
    "        cond_dim = None,\n",
    "        num_image_tokens = 4,\n",
    "        num_time_tokens = 2,\n",
    "        learned_sinu_pos_emb = True,\n",
    "        learned_sinu_pos_emb_dim = 16,\n",
    "        out_dim = None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        cond_images_channels = 0,\n",
    "        channels = 3,\n",
    "        channels_out = None,\n",
    "        attn_dim_head = 64,\n",
    "        attn_heads = 8,\n",
    "        ff_mult = 2.,\n",
    "        lowres_cond = False, # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
    "        layer_attns = True,\n",
    "        attend_at_middle = True, # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
    "        layer_cross_attns = True,\n",
    "        use_linear_attn = False,\n",
    "        use_linear_cross_attn = False,\n",
    "        cond_on_text = True,\n",
    "        max_text_len = 256,\n",
    "        init_dim = None,\n",
    "        init_conv_kernel_size = 7,\n",
    "        resnet_groups = 8,\n",
    "        init_cross_embed_kernel_sizes = (3, 7, 15),\n",
    "        cross_embed_downsample = False,\n",
    "        cross_embed_downsample_kernel_sizes = (2, 4),\n",
    "        attn_pool_text = True,\n",
    "        attn_pool_num_latents = 32,\n",
    "        dropout = 0.,\n",
    "        memory_efficient = False,\n",
    "        init_conv_to_final_conv_residual = False,\n",
    "        use_global_context_attn = True,\n",
    "        scale_resnet_skip_connection = True,\n",
    "        final_resnet_block = True,\n",
    "        final_conv_kernel_size = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # guide researchers\n",
    "\n",
    "        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n",
    "\n",
    "        if dim < 128:\n",
    "            print('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n",
    "\n",
    "        # save locals to take care of some hyperparameters for cascading DDPM\n",
    "\n",
    "        self._locals = locals()\n",
    "        self._locals.pop('self', None)\n",
    "        self._locals.pop('__class__', None)\n",
    "\n",
    "        # for eventual cascading diffusion\n",
    "\n",
    "        self.lowres_cond = lowres_cond\n",
    "\n",
    "\n",
    "        # determine dimensions\n",
    "\n",
    "        self.channels = channels\n",
    "        self.channels_out = default(channels_out, channels)\n",
    "\n",
    "        init_channels = channels if not lowres_cond else channels * 2 # in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n",
    "        init_dim = default(init_dim, dim)\n",
    "\n",
    "        # optional image conditioning\n",
    "\n",
    "        self.has_cond_image = cond_images_channels > 0\n",
    "        self.cond_images_channels = cond_images_channels\n",
    "\n",
    "        init_channels += cond_images_channels\n",
    "\n",
    "        # initial convolution\n",
    "\n",
    "        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # time conditioning\n",
    "\n",
    "        cond_dim = default(cond_dim, dim)\n",
    "        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n",
    "\n",
    "        # embedding time for discrete gaussian diffusion or log(snr) noise for continuous version\n",
    "\n",
    "        self.learned_sinu_pos_emb = learned_sinu_pos_emb\n",
    "\n",
    "        if learned_sinu_pos_emb:\n",
    "            sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n",
    "            sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n",
    "        else:\n",
    "            sinu_pos_emb = SinusoidalPosEmb(dim)\n",
    "            sinu_pos_emb_input_dim = dim\n",
    "\n",
    "        self.to_time_hiddens = nn.Sequential(\n",
    "            sinu_pos_emb,\n",
    "            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.to_time_cond = nn.Sequential(\n",
    "            nn.Linear(time_cond_dim, time_cond_dim)\n",
    "        )\n",
    "\n",
    "        # project to time tokens as well as time hiddens\n",
    "\n",
    "        self.to_time_tokens = nn.Sequential(\n",
    "            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n",
    "            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n",
    "        )\n",
    "\n",
    "        # low res aug noise conditioning\n",
    "\n",
    "        self.lowres_cond = lowres_cond\n",
    "\n",
    "        if lowres_cond:\n",
    "            self.to_lowres_time_hiddens = nn.Sequential(\n",
    "                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n",
    "                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n",
    "                nn.SiLU()\n",
    "            )\n",
    "\n",
    "            self.to_lowres_time_cond = nn.Sequential(\n",
    "                nn.Linear(time_cond_dim, time_cond_dim)\n",
    "            )\n",
    "\n",
    "            self.to_lowres_time_tokens = nn.Sequential(\n",
    "                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n",
    "                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n",
    "            )\n",
    "\n",
    "        # normalizations\n",
    "\n",
    "        self.norm_cond = nn.LayerNorm(cond_dim)\n",
    "\n",
    "        # text encoding conditioning (optional)\n",
    "\n",
    "        self.text_to_cond = None\n",
    "\n",
    "        if cond_on_text:\n",
    "            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n",
    "            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n",
    "\n",
    "        # finer control over whether to condition on text encodings\n",
    "\n",
    "        self.cond_on_text = cond_on_text\n",
    "\n",
    "        # attention pooling\n",
    "\n",
    "        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n",
    "\n",
    "        # for classifier free guidance\n",
    "\n",
    "        self.max_text_len = max_text_len\n",
    "\n",
    "        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n",
    "        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n",
    "\n",
    "        # for non-attention based text conditioning at all points in the network where time is also conditioned\n",
    "\n",
    "        self.to_text_non_attn_cond = None\n",
    "\n",
    "        if cond_on_text:\n",
    "            self.to_text_non_attn_cond = nn.Sequential(\n",
    "                nn.LayerNorm(cond_dim),\n",
    "                nn.Linear(cond_dim, time_cond_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_cond_dim, time_cond_dim)\n",
    "            )\n",
    "\n",
    "        # attention related params\n",
    "\n",
    "        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n",
    "\n",
    "        num_layers = len(in_out)\n",
    "\n",
    "        # resnet block klass\n",
    "\n",
    "        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n",
    "        resnet_groups = cast_tuple(resnet_groups, num_layers)\n",
    "\n",
    "        layer_attns = cast_tuple(layer_attns, num_layers)\n",
    "        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n",
    "\n",
    "        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n",
    "\n",
    "        # downsample klass\n",
    "\n",
    "        downsample_klass = Downsample\n",
    "        if cross_embed_downsample:\n",
    "            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n",
    "\n",
    "        # scale for resnet skip connections\n",
    "\n",
    "        skip_connect_scale = 1. if not scale_resnet_skip_connection else (2 ** -0.5)\n",
    "\n",
    "        # layers\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_cross_attns]\n",
    "        reversed_layer_params = list(map(reversed, layer_params))\n",
    "\n",
    "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(in_out, *layer_params)):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n",
    "            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n",
    "\n",
    "            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)\n",
    "\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                downsample_klass(dim_in, dim_out = dim_out) if memory_efficient else None,\n",
    "                ResnetBlock(dim_out if memory_efficient else dim_in, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups, skip_connection_scale = skip_connect_scale),\n",
    "                nn.ModuleList([ResnetBlock(dim_out, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn, skip_connection_scale = skip_connect_scale) for _ in range(layer_num_resnet_blocks)]),\n",
    "                transformer_block_klass(dim = dim_out, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),\n",
    "                downsample_klass(dim_out) if not memory_efficient and not is_last else None,\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "\n",
    "        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n",
    "        self.mid_attn = EinopsToAndFrom('b c h w', 'b (h w) c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n",
    "        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n",
    "\n",
    "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n",
    "            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n",
    "            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else nn.Identity)\n",
    "\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                ResnetBlock(dim_out * 2, dim_in, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups, skip_connection_scale = skip_connect_scale),\n",
    "                nn.ModuleList([ResnetBlock(dim_in, dim_in, time_cond_dim = time_cond_dim, groups = groups, skip_connection_scale = skip_connect_scale, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n",
    "                transformer_block_klass(dim = dim_in, heads = attn_heads, dim_head = attn_dim_head, ff_mult = ff_mult),\n",
    "                Upsample(dim_in) if not is_last or memory_efficient else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        # whether to do a final residual from initial conv to the final resnet block out\n",
    "\n",
    "        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n",
    "        final_conv_dim = dim * (2 if init_conv_to_final_conv_residual else 1)\n",
    "\n",
    "        # final optional resnet block and convolution out\n",
    "\n",
    "        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], skip_connection_scale = 1., use_gca = True) if final_resnet_block else None\n",
    "\n",
    "        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n",
    "        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n",
    "\n",
    "    # if the current settings for the unet are not correct\n",
    "    # for cascading DDPM, then reinit the unet with the right settings\n",
    "    def cast_model_parameters(\n",
    "        self,\n",
    "        *,\n",
    "        lowres_cond,\n",
    "        text_embed_dim,\n",
    "        channels,\n",
    "        channels_out,\n",
    "        cond_on_text,\n",
    "        learned_sinu_pos_emb\n",
    "    ):\n",
    "        if lowres_cond == self.lowres_cond and \\\n",
    "            channels == self.channels and \\\n",
    "            cond_on_text == self.cond_on_text and \\\n",
    "            text_embed_dim == self._locals['text_embed_dim'] and \\\n",
    "            learned_sinu_pos_emb == self.learned_sinu_pos_emb and \\\n",
    "            channels_out == self.channels_out:\n",
    "            return self\n",
    "\n",
    "        updated_kwargs = dict(\n",
    "            lowres_cond = lowres_cond,\n",
    "            text_embed_dim = text_embed_dim,\n",
    "            channels = channels,\n",
    "            channels_out = channels_out,\n",
    "            cond_on_text = cond_on_text,\n",
    "            learned_sinu_pos_emb = learned_sinu_pos_emb\n",
    "        )\n",
    "\n",
    "        return self.__class__(**{**self._locals, **updated_kwargs})\n",
    "\n",
    "    def forward_with_cond_scale(\n",
    "        self,\n",
    "        *args,\n",
    "        cond_scale = 1.,\n",
    "        **kwargs\n",
    "    ):\n",
    "        logits = self.forward(*args, **kwargs)\n",
    "\n",
    "        if cond_scale == 1:\n",
    "            return logits\n",
    "\n",
    "        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n",
    "        return null_logits + (logits - null_logits) * cond_scale\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        time,\n",
    "        *,\n",
    "        lowres_cond_img = None,\n",
    "        lowres_noise_times = None,\n",
    "        text_embeds = None,\n",
    "        text_mask = None,\n",
    "        cond_images = None,\n",
    "        cond_drop_prob = 0.\n",
    "    ):\n",
    "        batch_size, device = x.shape[0], x.device\n",
    "\n",
    "        # add low resolution conditioning, if present\n",
    "\n",
    "        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n",
    "        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n",
    "\n",
    "        if exists(lowres_cond_img):\n",
    "            x = torch.cat((x, lowres_cond_img), dim = 1)\n",
    "\n",
    "        # condition on input image\n",
    "\n",
    "        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n",
    "\n",
    "        if exists(cond_images):\n",
    "            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n",
    "            cond_images = resize_image_to(cond_images, x.shape[-1])\n",
    "            x = torch.cat((cond_images, x), dim = 1)\n",
    "\n",
    "        # initial convolution\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        # init conv residual\n",
    "\n",
    "        if self.init_conv_to_final_conv_residual:\n",
    "            init_conv_residual = x.clone()\n",
    "\n",
    "        # time conditioning\n",
    "\n",
    "        time_hiddens = self.to_time_hiddens(time)\n",
    "\n",
    "        # derive time tokens\n",
    "\n",
    "        time_tokens = self.to_time_tokens(time_hiddens)\n",
    "        t = self.to_time_cond(time_hiddens)\n",
    "\n",
    "        # add lowres time conditioning to time hiddens\n",
    "        # and add lowres time tokens along sequence dimension for attention\n",
    "\n",
    "        if self.lowres_cond:\n",
    "            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n",
    "            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n",
    "            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n",
    "\n",
    "            t = t + lowres_t\n",
    "            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n",
    "\n",
    "        # text conditioning\n",
    "\n",
    "        text_tokens = None\n",
    "\n",
    "        if exists(text_embeds) and self.cond_on_text:\n",
    "\n",
    "            # conditional dropout\n",
    "\n",
    "            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n",
    "\n",
    "            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n",
    "            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n",
    "\n",
    "            # calculate text embeds\n",
    "\n",
    "            text_tokens = self.text_to_cond(text_embeds)\n",
    "\n",
    "            text_tokens = text_tokens[:, :self.max_text_len]\n",
    "\n",
    "            text_tokens_len = text_tokens.shape[1]\n",
    "            remainder = self.max_text_len - text_tokens_len\n",
    "\n",
    "            if remainder > 0:\n",
    "                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n",
    "\n",
    "            if exists(text_mask):\n",
    "                if remainder > 0:\n",
    "                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n",
    "\n",
    "                text_mask = rearrange(text_mask, 'b n -> b n 1')\n",
    "                text_keep_mask_embed = text_mask & text_keep_mask_embed\n",
    "\n",
    "            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n",
    "\n",
    "            text_tokens = torch.where(\n",
    "                text_keep_mask_embed,\n",
    "                text_tokens,\n",
    "                null_text_embed\n",
    "            )\n",
    "\n",
    "            if exists(self.attn_pool):\n",
    "                text_tokens = self.attn_pool(text_tokens)\n",
    "\n",
    "            # extra non-attention conditioning by projecting and then summing text embeddings to time\n",
    "            # termed as text hiddens\n",
    "\n",
    "            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n",
    "\n",
    "            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n",
    "\n",
    "            null_text_hidden = self.null_text_hidden.to(t.dtype)\n",
    "\n",
    "            text_hiddens = torch.where(\n",
    "                text_keep_mask_hidden,\n",
    "                text_hiddens,\n",
    "                null_text_hidden\n",
    "            )\n",
    "\n",
    "            t = t + text_hiddens\n",
    "\n",
    "        # main conditioning tokens (c)\n",
    "\n",
    "        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n",
    "\n",
    "        # normalize conditioning tokens\n",
    "\n",
    "        c = self.norm_cond(c)\n",
    "\n",
    "        # go through the layers of the unet, down and up\n",
    "\n",
    "        hiddens = []\n",
    "\n",
    "        for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
    "            if exists(pre_downsample):\n",
    "                x = pre_downsample(x)\n",
    "\n",
    "            x = init_block(x, t, c)\n",
    "\n",
    "            for resnet_block in resnet_blocks:\n",
    "                x = resnet_block(x, t)\n",
    "\n",
    "            x = attn_block(x)\n",
    "            hiddens.append(x)\n",
    "\n",
    "            if exists(post_downsample):\n",
    "                x = post_downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t, c)\n",
    "\n",
    "        if exists(self.mid_attn):\n",
    "            x = self.mid_attn(x)\n",
    "\n",
    "        x = self.mid_block2(x, t, c)\n",
    "\n",
    "        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n",
    "            x = torch.cat((x, hiddens.pop()), dim = 1)\n",
    "            x = init_block(x, t, c)\n",
    "\n",
    "            for resnet_block in resnet_blocks:\n",
    "                x = resnet_block(x, t)\n",
    "\n",
    "            x = attn_block(x)\n",
    "            x = upsample(x)\n",
    "\n",
    "        # final top-most residual if needed\n",
    "\n",
    "        if self.init_conv_to_final_conv_residual:\n",
    "            x = torch.cat((x, init_conv_residual), dim = 1)\n",
    "\n",
    "        if exists(self.final_res_block):\n",
    "            x = self.final_res_block(x, t)\n",
    "\n",
    "        return self.final_conv(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
