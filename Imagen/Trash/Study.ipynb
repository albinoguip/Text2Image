{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec649f5e-aff1-4f97-a148-6663c8805d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple, Union, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0d61aa0-87fc-401f-b3fc-a45ae42c0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-10, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ff9b8a-ba22-45b9-874b-9faef32211c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fancy activation function\n",
    "class Swish(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83dfa4b5-8b2e-4c50-b8b5-49dcc8ec22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = nn.ReLU()\n",
    "y2 = r(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "598c4875-6770-40c2-b157-cef6e3e4976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Swish()\n",
    "y = s(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d5eccc5-a91f-4253-b608-c722113d9cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76308a5d-36b0-4d77-9ff8-b15a4fc28f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8397c6a-7d8a-4510-ad4c-4f71ed050e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m'''if time_dim >= 8 and time_dim < 16:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    raise ValueError(f'time_dim must be: time_dim < 8 or time_dim >= 16')'''\u001b[39;00m\n\u001b[0;32m      6\u001b[0m half_dim   \u001b[38;5;241m=\u001b[39m time_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m----> 8\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhalf_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(emb)\n\u001b[0;32m     10\u001b[0m emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39marange(half_dim) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39memb)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "time_dim = 15\n",
    "\n",
    "'''if time_dim >= 8 and time_dim < 16:\n",
    "    raise ValueError(f'time_dim must be: time_dim < 8 or time_dim >= 16')'''\n",
    "\n",
    "half_dim   = time_dim // 8\n",
    "\n",
    "emb = math.log(10_000) / (half_dim - 1)\n",
    "print(emb)\n",
    "emb = torch.exp(torch.arange(half_dim) * -emb)\n",
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59b0555a-493a-4330-b4a0-b3653b3144a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, time_dim : int, device = 'cpu'):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "        \n",
    "        self.time_dim = time_dim\n",
    "        self.half_dim = time_dim // 8\n",
    "        \n",
    "        ara = torch.arange(self.half_dim, device=device)\n",
    "        div = -(math.log(10_000) / (self.half_dim - 1))\n",
    "        \n",
    "        self.emb = torch.exp(ara * div)\n",
    "        \n",
    "        # Layers\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.time_dim // 4, self.time_dim)\n",
    "        self.linear2 = nn.Linear(self.time_dim, self.time_dim)\n",
    "        \n",
    "        self.swish   = Swish()\n",
    "        \n",
    "        print(self.emb[None, :].shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x[:, None] * self.emb[None, :]\n",
    "        print(out.shape)\n",
    "        out = self.swish(self.linear1(out))\n",
    "        \n",
    "        return self.linear2(out)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03f2de21-ff55-465b-89d8-0c76fa20ad45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
       "        0.9000])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.arange(0, 1, 0.1)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80a6c5e4-706a-4ea7-876d-3361490ab274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9909, 0.4458, 0.6178, 0.8257, 0.6781],\n",
       "        [0.2981, 0.6685, 0.5953, 0.0608, 0.4795],\n",
       "        [0.0478, 0.0929, 0.7673, 0.6236, 0.7600],\n",
       "        [0.9852, 0.7126, 0.6810, 0.7945, 0.6050],\n",
       "        [0.5724, 0.7059, 0.9505, 0.7144, 0.2210]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.rand((5,5))\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "986f2eb3-bb2c-4d9e-83f9-2ba70b05abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m TE \u001b[38;5;241m=\u001b[39m TimeEmbedding(\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mTE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ENV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36mTimeEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 25\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     27\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswish(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(out))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "TE = TimeEmbedding(20)\n",
    "y = TE(x_test)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fd226b7-deed-40f1-b1c2-3792aad7a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The time embedding \n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Embeddings for $t$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of dimensions in the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        # First linear layer\n",
    "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
    "        # Activation\n",
    "        self.act = Swish()\n",
    "        # Second linear layer\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        # Create sinusoidal position embeddings\n",
    "        # [same as those from the transformer](../../transformers/positional_encoding.html)\n",
    "        #\n",
    "        # \\begin{align}\n",
    "        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n",
    "        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n",
    "        # \\end{align}\n",
    "        #\n",
    "        # where $d$ is `half_dim`\n",
    "        half_dim = self.n_channels // 8\n",
    "        emb = math.log(10_000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "\n",
    "        #\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8aa4806-2e5a-4136-be54-600fb690df1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x24 and 25x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m TE \u001b[38;5;241m=\u001b[39m TimeEmbedding(\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mTE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ENV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36mTimeEmbedding.forward\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     34\u001b[0m emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((emb\u001b[38;5;241m.\u001b[39msin(), emb\u001b[38;5;241m.\u001b[39mcos()), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Transform with the MLP\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin1\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     38\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin2(emb)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ENV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ENV\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ENV\\lib\\site-packages\\torch\\nn\\functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x24 and 25x100)"
     ]
    }
   ],
   "source": [
    "TE = TimeEmbedding(100)\n",
    "y = TE(x_test)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e40f8164-4798-4d67-9906-97d243c6362b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976184"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40957d3d-f261-4f90-b1bf-ee80da1f9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The time embedding \n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Embeddings for $t$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of dimensions in the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        # First linear layer\n",
    "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
    "        # Activation\n",
    "        self.act = Swish()\n",
    "        # Second linear layer\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        \n",
    "        half_dim = self.n_channels // 8\n",
    "        emb = math.log(10_000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "\n",
    "        #\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7fdac3-6e00-4a0c-95dc-ebe9e1c5c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(      \n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.Swish())\n",
    "        \n",
    "        self.conv2 = nn.Sequential( \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.Swish())\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "            \n",
    "        self.linear = nn.Linear(time_dim, out_channels)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out += self.linear(t)[:, :, None, None]\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        \n",
    "        return out + self.shorcut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d457ebe2-baee-4a16-ae0f-3cceff04cd9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mcu\u001b[39;00m():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, p):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp \u001b[38;5;241m=\u001b[39m p\n",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36mcu\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, p):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mp:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuz\u001b[39m():\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpai\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "class cu():\n",
    "    \n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        \n",
    "    \n",
    "    if self.p:\n",
    "        def cuz():\n",
    "            print('pai')\n",
    "            \n",
    "    else:\n",
    "        def cuz():\n",
    "            print('mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d8128-5229-4c99-952b-14b7e03c56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual blocks include 'skip' connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Residual block\n",
    "    A residual block has two convolution layers with group normalization.\n",
    "    Each resolution is processed with two residual blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `in_channels` is the number of input channels\n",
    "        * `out_channels` is the number of input channels\n",
    "        * `time_channels` is the number channels in the time step ($t$) embeddings\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Group normalization and the first convolution layer\n",
    "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "        self.act1 = Swish()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # Group normalization and the second convolution layer\n",
    "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        self.act2 = Swish()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        # Linear layer for time embeddings\n",
    "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # First convolution layer\n",
    "        h = self.conv1(self.act1(self.norm1(x)))\n",
    "        # Add time embeddings\n",
    "        h += self.time_emb(t)[:, :, None, None]\n",
    "        # Second convolution layer\n",
    "        h = self.conv2(self.act2(self.norm2(h)))\n",
    "\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116c53e-6d28-40f8-b2fb-838e43dc616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahh yes, magical attention...\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Attention block\n",
    "    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of channels in the input\n",
    "        * `n_heads` is the number of heads in multi-head attention\n",
    "        * `d_k` is the number of dimensions in each head\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Default `d_k`\n",
    "        if d_k is None:\n",
    "            d_k = n_channels\n",
    "        # Normalization layer\n",
    "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
    "        # Projections for query, key and values\n",
    "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        # Linear layer for final transformation\n",
    "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
    "        # Scale for dot-product attention\n",
    "        self.scale = d_k ** -0.5\n",
    "        #\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        # Get shape\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
    "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
    "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = attn.softmax(dim=1)\n",
    "        # Multiply by values\n",
    "        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n",
    "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
    "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # Transform to `[batch_size, seq, n_channels]`\n",
    "        res = self.output(res)\n",
    "\n",
    "        # Add skip connection\n",
    "        res += x\n",
    "\n",
    "        # Change to shape `[batch_size, in_channels, height, width]`\n",
    "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "\n",
    "        #\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d7d7db0-fcc5-4200-935a-0fc43b14449c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k        = 5\n",
    "n_channels = 4\n",
    "\n",
    "m = d_k if d_k is not None else n_channels\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bcc6821a-e399-4872-ad2c-aaf88d29de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAttention(nn.Module):\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
    "\n",
    "        \n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k     = d_k if d_k is not None else n_channels\n",
    "        self.scale   = d_k ** -0.5\n",
    "        \n",
    "        # self.norm_layer   = nn.BatchNorm2d(n_channels)\n",
    "        self.linear_layer = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        self.output_layer = nn.Linear(n_heads * d_k, n_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
    "\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        \n",
    "        _ = t\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        \n",
    "        QKV = self.linear_layer(x)\n",
    "        QKV = QKV.view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        \n",
    "        Q, K, V = torch.chunk(QKV, 3, dim=-1)\n",
    "        \n",
    "        att = (torch.einsum('BIHD, BJHD -> BIJH', Q, K) * self.scale).softmax(dim=1)\n",
    "        \n",
    "        out = torch.einsum('BIJH, BJHD -> BIHD', att, V)\n",
    "        out = out.reshape(batch_size, -1, self.n_heads*self.d_k)\n",
    "        out = self.output_layer(out)\n",
    "        out += x\n",
    "        \n",
    "        return out.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e0035bf-840e-4a23-a0cc-bb3f17035134",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Attention(3, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d9302147-35fd-4102-8940-aba748238bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5, 4])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((1, 3, 5, 4))\n",
    "a(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbad71-5523-4d00-bf46-dad1e2f51862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahh yes, magical attention...\n",
    "class Attention(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
    "\n",
    "        \n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k     = d_k if d_k is not None else n_channels\n",
    "        self.scale   = d_k ** -0.5\n",
    "        \n",
    "        self.norm_layer   = nn.BatchNorm2d(n_channels)\n",
    "        self.linear_layer = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        self.output_layer = nn.Linear(n_heads * d_k, n_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        # Get shape\n",
    "        \n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        \n",
    "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
    "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
    "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = attn.softmax(dim=1)\n",
    "        # Multiply by values\n",
    "        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n",
    "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
    "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # Transform to `[batch_size, seq, n_channels]`\n",
    "        res = self.output(res)\n",
    "\n",
    "        # Add skip connection\n",
    "        res += x\n",
    "\n",
    "        # Change to shape `[batch_size, in_channels, height, width]`\n",
    "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "\n",
    "        #\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f807d0-496d-49cd-9eb1-5bf1b45a29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, time_dim, att):\n",
    "        super(UNetEncoder, self).__init__()\n",
    "        \n",
    "        self.conv = DoubleConv(in_channels, out_channels, time_dim)        \n",
    "        self.att  = ImageAttention(out_channels) if att else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        return self.att(self.conv(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e36f29-c31e-445a-a075-3923ae30a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Down block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c17f6-5c5d-44f1-836c-559fc84d3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, time_dim, att):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        \n",
    "        self.conv = DoubleConv(in_channels + out_channels, out_channels, time_dim)        \n",
    "        self.att  = ImageAttention(out_channels) if att else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        return self.att(self.conv(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c825c1a-33ba-4d84-94fc-4f157609d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Up block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace9413-f085-4aef-9150-6b5025cb6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetBottleneck(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, time_dim):\n",
    "        super(UNetBottleneck, self).__init__()\n",
    "        \n",
    "        self.conv1 = DoubleConv(in_channels, in_channels, time_dim) \n",
    "        self.conv2 = DoubleConv(in_channels, in_channels, time_dim) \n",
    "        self.att   = ImageAttention(in_channels)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        out = self.att(self.conv1(x, t))\n",
    "        return self.conv2(out, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd8306-b3b2-489e-a06f-8a2793047bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiddleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Middle block\n",
    "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n",
    "    This block is applied at the lowest resolution of the U-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, time_channels: int):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "        self.attn = AttentionBlock(n_channels)\n",
    "        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res1(x, t)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x, t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ef577-a555-4853-872b-d814bf056340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale up the feature map by $2 \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bd42c2-ff4f-4103-bb1b-d3a37c906cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale down the feature map by $\\frac{1}{2} \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d1269-f115-4b9a-9ed1-b29909ceaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
    "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "                 n_blocks: int = 2):\n",
    "       \n",
    "        super().__init__()\n",
    "        n_resolutions = len(ch_mults)\n",
    "        \n",
    "        self.down = nn.ModuleList()\n",
    "        self.up   = nn.ModuleList()\n",
    "        \n",
    "        out_channels = in_channels = n_channels\n",
    "        \n",
    "        self.image_transform = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.time_emb        = TimeEmbedding(n_channels * 4)\n",
    "        \n",
    "        for i in range(n_resolutions):\n",
    "            \n",
    "            out_channels = in_channels * ch_mults[i]            \n",
    "            for _ in range(n_blocks):\n",
    "                self.down.append(UNetEncoder(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            \n",
    "            if i < n_resolutions - 1:\n",
    "                self.down.append(nn.Conv2d(in_channels, in_channels, (3, 3), (2, 2), (1, 1)))\n",
    "                \n",
    "        self.middle = UNetBottleneck(out_channels, n_channels * 4)\n",
    "        \n",
    "        in_channels = out_channels\n",
    "        \n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            \n",
    "            out_channels = in_channels\n",
    "            \n",
    "            for _ in range(n_blocks):\n",
    "                self.up.append(UNetDecoder(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "                \n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            self.up.append(UNetDecoder(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            in_channels = out_channels\n",
    "            \n",
    "            \n",
    "            if i > 0:\n",
    "                self.up.append(nn.ConvTranspose2d(in_channels, in_channels, (4, 4), (2, 2), (1, 1)))\n",
    "                \n",
    "        self.norm = nn.BatchNorm2d(n_channels)\n",
    "        self.act = Swish()\n",
    "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Get time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, t)\n",
    "\n",
    "        # Second half of U-Net\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x, t)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x, t)\n",
    "\n",
    "        # Final normalization and convolution\n",
    "        return self.final(self.act(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3cda47-e90c-4831-9459-0008d3916dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core class definition (aka the important bit)\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ## U-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
    "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "                 n_blocks: int = 2):\n",
    "        \"\"\"\n",
    "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
    "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
    "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
    "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
    "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        # Project image into feature map\n",
    "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # Time embedding layer. Time embedding has `n_channels * 4` channels\n",
    "        self.time_emb = TimeEmbedding(n_channels * 4)\n",
    "\n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        \n",
    "        \n",
    "        for i in range(n_resolutions):\n",
    "            \n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            \n",
    "            for _ in range(n_blocks):\n",
    "                down.append(DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            \n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, n_channels * 4, )\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        # Final normalization and convolution layer\n",
    "        self.norm = nn.GroupNorm(8, n_channels)\n",
    "        self.act = Swish()\n",
    "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Get time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, t)\n",
    "\n",
    "        # Second half of U-Net\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x, t)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x, t)\n",
    "\n",
    "        # Final normalization and convolution\n",
    "        return self.final(self.act(self.norm(x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
